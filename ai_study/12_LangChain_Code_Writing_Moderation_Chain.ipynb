{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zDgLTgesutN2"
      },
      "source": [
        "# 1. PythonREPL을 이용한 Code Writing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wbeOsSHBjqsC"
      },
      "source": [
        "Python REPL(Read-Eval-Print Loop)사용자가 Python 코드를 입력하고 실행하며 결과를 즉시 확인할 수 있는 기능을제공\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zQaO43GPlBaT"
      },
      "source": [
        "Read-Eval-Print Loop(읽기-평가-출력 루프)는 다음과 같은 세 가지 단계를 반복하는 과정을 의미합니다:\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KKeaQm7mlT2W"
      },
      "source": [
        "\n",
        "Read(읽기): 사용자가 입력한 코드를 읽고 해석합니다.\n",
        "\n",
        "Eval(평가): 읽어들인 코드를 실행(평가)하여 결과를 계산합니다.\n",
        "\n",
        "Print(출력): 평가된 결과를 화면에 출력합니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JupOD2wFSjYE"
      },
      "source": [
        "LangChain 기반 자동 코드 생성 + 실행 + 모더레이션 체계 설명\n",
        "\n",
        "🧠 전체 개요\n",
        "\n",
        "이 실습은 LangChain + OpenAI API를 활용하여:\n",
        "\n",
        "사용자의 문제를 읽고\n",
        "\n",
        "그에 맞는 Python 코드를 자동으로 생성한 다음\n",
        "\n",
        "생성된 코드를 즉시 실행하고 결과를 보여주며\n",
        "\n",
        "유해하거나 부적절한 콘텐츠는 자동으로 차단하는 지능형 시스템을 구현한 것입니다.\n",
        "\n",
        "즉, 마치 \"AI 코딩 비서\"처럼 동작하는 시스템입니다!\n",
        "\n",
        "🧩 구성 요소별 큰 그림 설명\n",
        "\n",
        "🧱 1. 사용자 입력 → 프롬프트 생성\n",
        "\n",
        "사용자가 자연어로 질문을 던지면 (\"x가 10일 때, y=5+3x는?\")\n",
        "\n",
        "이걸 바탕으로 AI가 코드 생성을 위한 프롬프트(지침 문장)을 구성합니다.\n",
        "\n",
        "👉 직관적인 비유: 사용자의 질문은 \"문제지\", 프롬프트는 \"선생님의 지시사항\"입니다.\n",
        "\n",
        "🤖 2. LLM(GPT)을 통해 코드 생성\n",
        "GPT 모델(GPT-4나 GPT-4o 등)이 지침에 따라 파이썬 코드만 생성합니다.\n",
        "\n",
        "y = 5 + 3 * x 같은 식을 자동으로 만들어 줍니다.\n",
        "\n",
        "👉 비유: AI가 문제를 읽고, 스스로 파이썬 코드를 손으로 써내려 가는 셈입니다.\n",
        "\n",
        "⚙️ 3. 코드 실행 (Python REPL)\n",
        "만들어진 코드는 곧바로 파이썬 실행기(Python REPL)를 통해 돌려봅니다.\n",
        "\n",
        "결과가 변수 result에 담기고 print(result)로 출력됩니다.\n",
        "\n",
        "👉 비유: AI가 작성한 코드를 바로 실행해보며, \"답이 맞는지\" 체크하는 겁니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AZcdLsf5lGBe"
      },
      "source": [
        "### Code Writing1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l_Z_rhroUqVw"
      },
      "source": [
        "# 📘 LLM을 활용한 파이썬 코드 자동 생성 및 실행 흐름 설명\n",
        "\n",
        "이 강의에서는 대화형 인공지능 모델인 LLM(Language Model)을 활용해,  \n",
        "사용자가 자연어로 입력한 질문을 자동으로 파이썬 코드로 바꾸고,  \n",
        "그 코드를 실행하여 결과까지 출력하는 전체 과정을 다룹니다.  \n",
        "처음 접하는 분도 이해할 수 있도록 차근차근 설명드리겠습니다.\n",
        "\n",
        "---\n",
        "\n",
        "## 1. 필수 라이브러리 설치 및 API 키 설정\n",
        "\n",
        "가장 먼저, LangChain과 OpenAI API를 활용하기 위한 필수 도구들을 설치합니다.  \n",
        "그리고 OpenAI 모델에 접속하기 위한 API 키를 환경 변수로 등록합니다.  \n",
        "이 키는 \"이 사용자에게 모델 사용 권한이 있다\"는 일종의 인증 수단입니다.\n",
        "\n",
        "---\n",
        "\n",
        "## 2. GPT에게 명확한 역할 지시: 오직 코드만 생성하라!\n",
        "\n",
        "GPT는 매우 유연한 언어 모델이기 때문에 우리가 원하는 결과를 얻으려면,  \n",
        "**정확한 지시문(prompt)**이 필요합니다.  \n",
        "여기서는 “문제 해결을 위한 파이썬 코드만 작성하고, 결과는 print(result)로 출력하라”는 명령을 줍니다.\n",
        "\n",
        "이러한 지시문은 모델에게 “너는 지금 코드만 생성하는 역할을 해”라고 명확히 알려주는 역할을 합니다.\n",
        "\n",
        "---\n",
        "\n",
        "## 3. 사용자 질문과 시스템 지시문을 결합\n",
        "\n",
        "예를 들어 사용자가 “x가 10일 때, y = 5 + 3x는 얼마인가요?”라고 묻는다면,  \n",
        "GPT는 우리가 설정한 지시문에 따라 다음과 같은 코드를 생성할 것입니다:\n",
        "\n",
        "```python\n",
        "x = 10\n",
        "y = 5 + 3 * x\n",
        "result = y\n",
        "print(result)\n",
        " 4. 생성된 코드를 즉시 실행하기\n",
        "이제 LangChain의 PythonREPL 기능을 활용해 GPT가 만든 코드를 바로 실행할 수 있습니다.\n",
        "REPL(Read-Eval-Print-Loop)은 파이썬 코드를 읽고, 평가하고, 결과를 출력하는 구조입니다.\n",
        "\n",
        "즉, 사용자는 질문만 하고,\n",
        "GPT가 코드 생성 → LangChain이 실행 → 결과 출력\n",
        "이 모든 과정을 자동으로 처리합니다.\n",
        "\n",
        "5. 고도화된 체인 구성 (코드 추출 포함)\n",
        "GPT는 때때로 코드 외의 설명을 함께 반환할 수 있습니다.\n",
        "이를 방지하기 위해 extract_code() 함수를 사용하여 코드 블록만 분리해냅니다.\n",
        "이후 이 코드만을 실행기로 전달해 결과를 출력합니다.\n",
        "\n",
        "또한, 이 과정에서는 최신 모델인 GPT-4o를 사용하여 더 빠르고 저렴한 처리를 도입합니다.\n",
        "\n",
        "6. 예시: 원주율을 소수점 이하 30자리까지 구하기\n",
        "사용자가 \"원주율을 소수점 이하 30자리까지 정확히 구해 주세요\"라고 요청하면,\n",
        "GPT는 다음과 같은 고정 소수점 계산 코드를 생성합니다:\n",
        "\n",
        "python\n",
        "복사\n",
        "편집\n",
        "from decimal import Decimal, getcontext\n",
        "getcontext().prec = 35\n",
        "pi = Decimal(16)*Decimal('0.2').atan() - Decimal(4)*Decimal('0.5').atan()\n",
        "result = pi\n",
        "print(result)\n",
        "실행하면, 매우 정확한 원주율 값을 출력합니다.\n",
        "\n",
        "✅ 요약: 왜 이게 중요한가요?\n",
        "사용자 → 자연어 질문\n",
        "\n",
        "GPT → 파이썬 코드 자동 생성\n",
        "\n",
        "LangChain → 코드 실행 및 결과 반환\n",
        "\n",
        "이 구조 덕분에 하드코딩 없이 다양한 데이터 분석과 계산을 자연어로 처리할 수 있습니다.\n",
        "즉, 개발 지식이 부족해도 AI의 도움을 받아 자유롭게 프로그래밍 작업을 수행할 수 있는 환경이 만들어지는 것입니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wgBxBi0Cu2Nb",
        "outputId": "7f9e0611-4520-4f5d-e76d-60149e6bef32"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/209.2 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m \u001b[32m204.8/209.2 kB\u001b[0m \u001b[31m30.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.2/209.2 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/63.4 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.4/63.4 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m23.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m438.3/438.3 kB\u001b[0m \u001b[31m18.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.4/44.4 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "# 📌 필요한 라이브러리를 설치합니다. LangChain 및 OpenAI 연동을 위한 핵심 패키지입니다.\n",
        "!pip install langchain langchain-experimental langchain-openai -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "jBofuZ8gkRK5"
      },
      "outputs": [],
      "source": [
        "# 📌 os 모듈을 이용해 환경변수를 설정합니다.\n",
        "# ▶️ 포인트: OpenAI API 키는 외부에 노출되면 안 되므로 코드에 직접 입력하기보다는 환경변수를 사용합니다.\n",
        "import os\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J9gOk5m7j4ae",
        "outputId": "90f8e589-6906-4c17-9125-3417ddd28c63"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-3-851f7996425c>:21: LangChainDeprecationWarning: The class `ChatOpenAI` was deprecated in LangChain 0.0.10 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-openai package and should be used instead. To use it run `pip install -U :class:`~langchain-openai` and import as `from :class:`~langchain_openai import ChatOpenAI``.\n",
            "  model = ChatOpenAI(temperature=0, model_name=\"gpt-4\")  # temperature=0: 항상 일관된 결과 생성\n"
          ]
        }
      ],
      "source": [
        "# 📌 필요한 LangChain 컴포넌트들을 불러옵니다.\n",
        "from langchain.chat_models import ChatOpenAI  # OpenAI의 챗 기반 모델을 사용\n",
        "from langchain.prompts import ChatPromptTemplate  # 사용자 질문을 GPT에 맞는 형태로 바꿔주는 역할\n",
        "from langchain_core.output_parsers import StrOutputParser  # GPT 출력 텍스트를 파싱\n",
        "from langchain_experimental.utilities import PythonREPL  # 생성된 파이썬 코드를 직접 실행하는 기능 제공\n",
        "\n",
        "# 📌 GPT 모델에게 줄 시스템 메시지를 구성합니다.\n",
        "# ▶️ 포인트: 아래 템플릿은 \"오직 파이썬 코드만 출력하고, 결과는 result로 출력하라\"고 GPT에게 명령하는 지시문입니다.\n",
        "template = \"\"\"\n",
        "사용자의 문제를 해결하기 위한 파이썬 코드를 작성하세요.\n",
        "사용자가 결과를 확인할 수 있도록 코드의 끝에 print(result)를 포함하세요.\n",
        "오직 파이썬 코드만 반환하세요. 그 외의 것은 포함하지 마세요.\n",
        "\"\"\"\n",
        "\n",
        "# 📌 실제로 사용할 프롬프트 구조 정의: 시스템 템플릿 + 사용자 질문\n",
        "prompt = ChatPromptTemplate.from_messages(\n",
        "    [(\"system\", template), (\"human\", \"{question}\")]\n",
        ")\n",
        "\n",
        "# 📌 GPT-4 모델을 사용해 답변을 생성하도록 설정합니다.\n",
        "model = ChatOpenAI(temperature=0, model_name=\"gpt-4\")  # temperature=0: 항상 일관된 결과 생성\n",
        "\n",
        "# 📌 체인1: 질문 → 프롬프트 → GPT → 결과 파싱까지\n",
        "PythonCode_chain = prompt | model | StrOutputParser()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "-XTnHUDAkU2T",
        "outputId": "565fc7af-ceec-4b3e-af8c-d7fae23a8f29"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'x = 10\\ny = 5 + 3 * x\\nresult = y\\nprint(result)'"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# ▶️ 포인트: 아래는 실제로 질문을 입력하여 GPT가 코드를 생성하고, 결과를 실행하도록 요청하는 예시입니다.\n",
        "# 사용자가 질문합니다: \"x가 10일 때 y = 5 + 3 * x 이면 y는 얼마인가요?\"\n",
        "PythonCode_chain.invoke({\"question\": \"y= 5 + 3 * x. If x is 10, what is y?\"})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tm8-2ZrYkvwt",
        "outputId": "e48e5eb4-63e0-497e-b70c-a4279b03aa45"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "35\n"
          ]
        }
      ],
      "source": [
        "# ✅ GPT가 생성할 것으로 예상되는 코드 예시:\n",
        "x = 10\n",
        "y = 5 + 3 * x\n",
        "result = y\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "PwnWLcKckcqG",
        "outputId": "641525ce-580c-4cff-b57f-ed232eb58d3b"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:langchain_experimental.utilities.python:Python REPL can execute arbitrary code. Use with caution.\n"
          ]
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'35\\n'"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# ▶️ GPT가 위와 비슷한 코드를 자동 생성하고, Python REPL을 통해 실행한 결과를 반환합니다.\n",
        "PythonCodeRun_chain.invoke({\"question\": \"y= 5 + 3 * x. If x is 10, what is y?\"})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OYakIDyElJqO"
      },
      "source": [
        "# 2.CodeWriting"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XHVEUcFOmhQa"
      },
      "source": [
        "CodeWriting2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "u-jX0LpXo2D0"
      },
      "outputs": [],
      "source": [
        "# 📌 최신 버전으로 langchain 패키지 업그레이드\n",
        "!pip install --upgrade --quiet  langchain langchain-openai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "Alvf1JvzutN4"
      },
      "outputs": [],
      "source": [
        "# 📌 필요한 모듈 재호출\n",
        "from langchain_experimental.utilities import PythonREPL\n",
        "import os\n",
        "from langchain_openai import ChatOpenAI  # ✅ langchain-openai와 langchain.chat_models는 동일 목적이지만 분리된 패키지\n",
        "\n",
        "# 📌 다시 환경변수 설정 (필요 시 재실행)\n",
        "\n",
        "# 📌 시스템 지시문 템플릿을 구성합니다. 코드 블록을 반드시 포함하도록 설정합니다.\n",
        "template = \"\"\"사용자의 문제를 해결하기 위한 Python 코드를 작성하세요.\n",
        "\n",
        "다음과 같은 형태로 Python 코드만 반환하세요:\n",
        "\n",
        "```python\n",
        "....\n",
        "```\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "q47Gx06albu-"
      },
      "outputs": [],
      "source": [
        "# 📌 사용자 질문에 따라 프롬프트를 자동 구성\n",
        "prompt = ChatPromptTemplate.from_messages(\n",
        "    [(\"system\", template),\n",
        "     (\"human\", \"{input}\")])\n",
        "\n",
        "# 📌 더 빠르고 저렴한 GPT-4o 모델을 사용\n",
        "llm = ChatOpenAI(model='gpt-4o', temperature=0, max_tokens=1024)\n",
        "\n",
        "# 📌 GPT 응답에서 코드만 추출하는 함수 정의\n",
        "# ▶️ 예: GPT가 코드 외에도 부가 설명을 넣을 수 있기 때문에 ```python ~ ``` 블록 안의 코드만 가져옵니다.\n",
        "def extract_code(text):\n",
        "    return text.split('```python')[1].split(\"```\")[0]  # 코드만 깔끔하게 분리"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CADRkuuFlc8j",
        "outputId": "755af081-40f4-4e55-c58c-b5da05cf4826"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "12\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# 📌 텍스트 출력을 담당하는 파서 모듈\n",
        "from langchain.schema.output_parser import StrOutputParser\n",
        "\n",
        "# 📌 전체 체인 구성:\n",
        "# 질문 → 프롬프트 → GPT 응답 → 코드만 추출 → 코드 실행(Python REPL)\n",
        "chain = prompt | llm | StrOutputParser() | extract_code | PythonREPL().run\n",
        "\n",
        "# ✅ 실습 예시: 원주율을 소수점 이하 30자리까지 구하는 코드를 GPT가 생성하고 실행합니다.\n",
        "print(chain.invoke({'input': \"4 곱하기 3을 구해주세요\"}))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ypOZ21GUlqa2",
        "outputId": "dd250fc6-76b4-4af1-d834-13c8adab328b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "result = 4 * 3\n",
            "print(result)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# 📌 위와 동일하지만, '코드 생성'까지만 수행하는 별도 체인\n",
        "code_gen = prompt | llm | StrOutputParser() | extract_code\n",
        "\n",
        "# ✅ 같은 질문에 대해, 생성된 코드만 보고 싶을 때 사용하는 예시\n",
        "print(code_gen.invoke({'input': \"4곱하기 3을 구해주세요\"}))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WCL4UKafutN5"
      },
      "source": [
        "# 2. Moderation 기능"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N3efEmdfmLJB"
      },
      "source": [
        "(공식문서)\n",
        "https://platform.openai.com/docs/guides/moderation/overview?lang=python"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "59Qo-SBhp7_E"
      },
      "source": [
        "# Moderation1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r41dm02Tp-7-"
      },
      "source": [
        "혐오 (hate): 인종, 성별, 민족, 종교, 국적, 성적 지향, 장애 상태 또는 카스트에 기반하여 혐오를 표현하거나 선동하거나 조장하는 콘텐츠. 보호되지 않는 그룹(예: 체스 선수)에 대한 혐오 콘텐츠는 괴롭힘으로 간주됩니다.\n",
        "\n",
        "혐오/위협 (hate/threatening): 인종, 성별, 민족, 종교, 국적, 성적 지향, 장애 상태 또는 카스트에 기반하여 해당 그룹에 대한 폭력 또는 심각한 피해를 포함하는 혐오 콘텐츠.\n",
        "\n",
        "괴롭힘 (harassment): 특정 대상에 대한 괴롭힘 언어를 표현하거나 선동하거나 조장하는 콘텐츠.\n",
        "\n",
        "괴롭힘/위협 (harassment/threatening): 특정 대상에 대한 폭력 또는 심각한 피해를 포함하는 괴롭힘 콘텐츠.\n",
        "\n",
        "자해 (self-harm): 자살, 자해(베기), 섭식 장애와 같은 자해 행위를 조장하거나 권장하거나 묘사하는 콘텐츠.\n",
        "\n",
        "자해/의도 (self-harm/intent): 화자가 자살, 자해(베기), 섭식 장애와 같은 자해 행위를 하고 있거나 하려고 한다고 표현하는 콘텐츠.\n",
        "\n",
        "자해/방법 (self-harm/instructions): 자살, 자해(베기), 섭식 장애와 같은 자해 행위를 하도록 권장하거나 이러한 행위를 수행하는 방법 또는 조언을 제공하는 콘텐츠.\n",
        "\n",
        "성적인 (sexual): 성적 흥분을 유발하려는 목적을 가진 콘텐츠, 성적 활동을 묘사하거나 성적 서비스를 홍보하는 콘텐츠(성 교육 및 웰니스 제외).\n",
        "\n",
        "성적인/미성년자 (sexual/minors): 18세 미만의 개인이 포함된 성적 콘텐츠.\n",
        "\n",
        "폭력 (violence): 죽음, 폭력 또는 신체적 상해를 묘사하는 콘텐츠.\n",
        "\n",
        "폭력/상세 (violence/graphic): 죽음, 폭력 또는 신체적 상해를 상세히 묘사하는 콘텐츠."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sdyn6fP_q0A7"
      },
      "source": [
        " 값은 0과 1 사이이며, 값이 높을수록 모델의 확신도가 높습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0-x-BoKeqE3k",
        "outputId": "6781ef64-b428-4d5a-d71e-785c0aa388e3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'harassment': 0.00025651900796219707, 'harassment_threatening': 0.0017967631574720144, 'hate': 0.000131044042063877, 'hate_threatening': 3.330395702505484e-05, 'illicit': None, 'illicit_violent': None, 'self_harm': 0.10243354737758636, 'self_harm_instructions': 0.012842289172112942, 'self_harm_intent': 0.06416939944028854, 'sexual': 5.647746365866624e-05, 'sexual_minors': 8.965668712335173e-06, 'violence': 0.01611601747572422, 'violence_graphic': 0.00012667976261582226, 'self-harm': 0.10243354737758636, 'sexual/minors': 8.965668712335173e-06, 'hate/threatening': 3.330395702505484e-05, 'violence/graphic': 0.00012667976261582226, 'self-harm/intent': 0.06416939944028854, 'self-harm/instructions': 0.012842289172112942, 'harassment/threatening': 0.0017967631574720144}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-13-4e7e1e4456ca>:15: PydanticDeprecatedSince20: The `dict` method is deprecated; use `model_dump` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.11/migration/\n",
            "  scores = response.category_scores.dict()\n"
          ]
        }
      ],
      "source": [
        "import openai\n",
        "# OpenAI API 클라이언트 초기화\n",
        "client = openai.OpenAI()\n",
        "\n",
        "# 분석할 텍스트 정의\n",
        "text = '자살'\n",
        "\n",
        "# 텍스트에 대한 콘텐츠 검토 요청을 생성하고, 첫 번째 결과를 가져옴\n",
        "response = client.moderations.create(input=text).results[0]\n",
        "\n",
        "# 텍스트가 부적절하거나 유해한 내용으로 플래그된 여부를 확인\n",
        "flagged = response.flagged\n",
        "\n",
        "# 텍스트에 대한 카테고리별 점수들을 딕셔너리로 변환\n",
        "scores = response.category_scores.dict()\n",
        "\n",
        "# 카테고리별 점수를 출력\n",
        "print(scores)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "Kojp4Zp8utN5"
      },
      "outputs": [],
      "source": [
        "import openai\n",
        "# OpenAI API 클라이언트 초기화\n",
        "client = openai.OpenAI()\n",
        "\n",
        "# **Moderation 함수**\n",
        "# 주어진 텍스트가 커뮤니티 규약을 위반하는지 확인하고, 필요시 응답을 거부합니다.\n",
        "def moderate(text):\n",
        "    # 주어진 텍스트에 대해 모더레이션(콘텐츠 검토) 요청을 보내고 결과를 가져옴\n",
        "    response = client.moderations.create(input=text).results[0]\n",
        "\n",
        "    # 텍스트가 플래그되었는지(유해하거나 부적절한지) 확인\n",
        "    flagged = response.flagged\n",
        "\n",
        "    # 각 카테고리에 대한 점수를 딕셔너리로 변환\n",
        "    scores = response.category_scores.dict()\n",
        "\n",
        "    # 위반 카테고리 중 가장 높은 점수를 가진 항목 찾기\n",
        "    max_category = max(scores, key=scores.get)  # 점수가 가장 높은 카테고리 이름\n",
        "    max_score = scores[max_category]  # 그 카테고리의 점수\n",
        "\n",
        "    # 모더레이션 결과에 따라 응답을 생성\n",
        "    if flagged or max_score > 0.000000001:\n",
        "        # 플래그되었거나 특정 카테고리에서 높은 점수를 받았다면 응답을 차단\n",
        "        return {'blocked': True, 'reason': max_category, 'score': max_score}\n",
        "\n",
        "    # 그렇지 않다면 텍스트를 그대로 반환\n",
        "    return {'blocked': False, 'text': text}\n",
        "\n",
        "# **LLM 응답 처리 체인**\n",
        "# 사용자가 입력한 텍스트를 기반으로 적절한 응답을 반환하는 함수입니다.\n",
        "def generate_response(prompt):\n",
        "    # 입력된 텍스트를 모더레이션 함수에 전달하여 검토\n",
        "    moderation_result = moderate(prompt)\n",
        "\n",
        "    # 모더레이션 결과에 따라 응답을 결정\n",
        "    if moderation_result['blocked']:\n",
        "        print(\"모더레이션 실패!\")  # 모더레이션에서 차단된 경우\n",
        "        return f\"죄송합니다. 해당 내용은 유해한 내용입니다. 사유: {moderation_result['reason']}\"\n",
        "    else:\n",
        "        print(\"모더레이션 통과.\")  # 모더레이션을 통과한 경우\n",
        "        # LLM 응답 생성 로직 (여기서는 예시로 출력)\n",
        "        return f\"사용자 입력: {moderation_result['text']}\"\n",
        "\n",
        "# **Fallback 처리**\n",
        "# 기본 응답 체인이 실패할 경우 실행되는 대체 응답 로직입니다.\n",
        "def fallback_response(fallback_msg):\n",
        "    print(\"Fallback 실행 중...\")\n",
        "    return \"그런 대화에는 대답할 수 없어요.\"\n",
        "\n",
        "# **최종 응답 체인**\n",
        "# 기본 응답 체인과 Fallback 체인을 결합하여 최종 응답을 처리합니다.\n",
        "def respond(text):\n",
        "    try:\n",
        "        # 기본 응답 생성 로직 실행\n",
        "        response = generate_response(text)\n",
        "    except Exception as e:\n",
        "        # 오류 발생 시 Fallback 응답 생성 로직 실행\n",
        "        print(f\"오류 발생: {e}\")\n",
        "        response = fallback_response(text)\n",
        "\n",
        "    # 최종 응답 반환\n",
        "    return response\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P6VtaCS8wE8W",
        "outputId": "36b1f2d4-478c-4f82-eb38-8c966794b2f3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "오류 발생: '>' not supported between instances of 'NoneType' and 'float'\n",
            "Fallback 실행 중...\n",
            "그런 대화에는 대답할 수 없어요.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-14-20f629999668>:15: PydanticDeprecatedSince20: The `dict` method is deprecated; use `model_dump` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.11/migration/\n",
            "  scores = response.category_scores.dict()\n"
          ]
        }
      ],
      "source": [
        "# 테스트: 정상적인 입력과 위반된 입력 처리\n",
        "print(respond(\"실패\"))           # 모더레이션 실패 예시\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
