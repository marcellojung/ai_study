{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fJrjiEjCdoL9"
   },
   "source": [
    "# 벡터데이터 기반 RAG 어플리케이션 실습"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k4JMyYuhn9eG"
   },
   "source": [
    "🚀 목표는 뭘까?\n",
    "RAG (Retrieval-Augmented Generation) 시스템을 만들기 위한 실습입니다.\n",
    "즉, 웹에서 정보를 가져오고 → 그 내용을 기억하도록 정리(벡터화)한 다음 → 사용자 질문에 그 정보를 활용해 똑똑하게 대답하는 AI 챗봇을 만드는 겁니다.\n",
    "\n",
    "📦 전체 흐름 요약\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "1.웹 페이지에서 정보 가져오기\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "마치 구글에서 웹문서를 복사해오는 것처럼, 웹에서 필요한 문서를 자동으로 긁어옵니다.\n",
    "\n",
    "---\n",
    "2.문서를 잘게 나누기 (텍스트 분할)\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "문서를 한 덩어리로 두면 AI가 이해하기 어려우니까, 문장을 일정 크기로 슬라이스해줍니다.\n",
    "---\n",
    "3.벡터 임베딩을 생성하고 저장하기\n",
    "---\n",
    "잘라낸 문장들을 숫자로 바꿔 기억하는 단계입니다.\n",
    "\n",
    "즉, 문장을 **벡터(숫자 리스트)**로 바꿔 저장소에 저장합니다.\n",
    "---\n",
    "4.질문이 들어오면 관련 문장 찾기\n",
    "---\n",
    "사용자가 질문을 던지면, 아까 저장한 벡터 중 가장 관련 있는 문장을 찾아옵니다.\n",
    "---\n",
    "5.찾아온 문장 + 질문을 함께 LLM(GPT)에 전달하기\n",
    "---\n",
    "\"이 문장들을 참고해서 이 질문에 답해줘!\" 하고 GPT에게 건네주는 역할입니다.\n",
    "---\n",
    "6.GPT가 정답을 만들어냄\n",
    "---\n",
    "최종적으로 AI가 위 내용을 바탕으로 짧고 정확한 답변을 생성합니다.\n",
    "\n",
    "🎯 쉽게 비유하자면...\n",
    "\n",
    " 🔍 웹 문서는 책장에 꽂힌 책이고\n",
    "\n",
    " ✂️ 텍스트 분할은 책을 챕터별로 자르는 작업이고\n",
    "\n",
    " 🧠 벡터화는 챕터를 AI가 이해할 수 있도록 기억시키는 것이고\n",
    "\n",
    " ❓ 사용자의 질문은 AI에게 \"이 책에서 ~~내용 알려줘\" 하는 것\n",
    "\n",
    " 🤖 GPT는 책을 뒤져서 정확한 답을 말해주는 똑똑한 사서입니다.\n",
    "\n",
    "\n",
    "단계\t설명\n",
    "---\n",
    "1단계\t웹 문서 긁어오기\n",
    "---\n",
    "2단계\t텍스트 슬라이스(1000자씩 자르기)\n",
    "---\n",
    "3단계\t잘린 텍스트 → 숫자(벡터)로 바꿔 저장\n",
    "---\n",
    "4단계\t질문 → 관련된 텍스트 검색\n",
    "---\n",
    "5단계\t질문 + 문맥 → GPT에 전달\n",
    "---\n",
    "6단계\tGPT → 최종 응답 생성\n",
    "---\n",
    "\n",
    "💡 이런 걸 왜 쓰나요?\n",
    "단순히 GPT에게 질문하면 모르는 척하거나 헛소리를 할 수 있습니다.\n",
    "\n",
    "RAG는 GPT가 실제 자료를 참고해서 답하게 도와줍니다.\n",
    "\n",
    "즉, GPT를 지식 기반 AI로 한 단계 업그레이드시키는 방법입니다!\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bEXZHs6GdsA3"
   },
   "outputs": [],
   "source": [
    "# 필요한 외부 라이브러리를 설치합니다.\n",
    "# 포인트: 이 라이브러리들은 GPT와 외부 데이터를 연결하고 처리하는 데 꼭 필요합니다.\n",
    "!pip install --upgrade openai langchain langchain-openai langchain-community beautifulsoup4 langchainhub -q\n",
    "!pip install chromadb tiktoken -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1383,
     "status": "ok",
     "timestamp": 1728648965637,
     "user": {
      "displayName": "이진규",
      "userId": "17496981203379941373"
     },
     "user_tz": -540
    },
    "id": "x13Lo_xEg4W6",
    "outputId": "97f98bb5-9671-4211-b366-69ad559efc78"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "API 키 설정이 완료되었습니다.\n"
     ]
    }
   ],
   "source": [
    "# openai 모듈과 os 모듈을 불러옵니다. (os는 환경 변수 설정에 필요)\n",
    "import openai\n",
    "import os\n",
    "\n",
    "# OpenAI API 키를 환경 변수로 등록합니다.\n",
    "# 포인트: 이렇게 하면 코드에 키가 노출되지 않고 안전하게 사용할 수 있습니다.\n",
    "\n",
    "# OpenAI API와 연결합니다.\n",
    "client = openai.OpenAI()\n",
    "print(\"API 키 설정이 완료되었습니다.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5382,
     "status": "ok",
     "timestamp": 1728648971016,
     "user": {
      "displayName": "이진규",
      "userId": "17496981203379941373"
     },
     "user_tz": -540
    },
    "id": "gKsItoP-yNHO",
    "outputId": "82dcd3fe-dc88-4c6a-86c6-c406138a0ce7"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:langchain_community.utils.user_agent:USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    }
   ],
   "source": [
    "# LangChain과 BeautifulSoup 관련 라이브러리 불러오기\n",
    "import os\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter  # 문서 자를 때 사용\n",
    "from langchain_community.document_loaders import WebBaseLoader  # 웹 문서 로드\n",
    "from langchain_community.vectorstores import Chroma  # 벡터 DB 저장소\n",
    "from langchain_core.output_parsers import StrOutputParser  # 출력 형식 변환기\n",
    "from langchain_core.runnables import RunnablePassthrough  # 입력 그대로 넘겨주는 모듈\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings  # GPT 모델 및 임베딩 생성\n",
    "import bs4\n",
    "\n",
    "# 웹에서 문서를 가져오는 WebBaseLoader를 설정합니다.\n",
    "# 포인트: 지정한 웹 URL에서 특정 HTML 클래스에 해당하는 본문 내용만 추출하도록 합니다.\n",
    "loader = WebBaseLoader(\n",
    "    web_paths=(\"https://mlrx.tistory.com/entry/%EC%B1%97%EB%B4%87-PDF-QA-%EC%B1%97%EB%B4%87-%EA%B0%9C%EB%B0%9C%ED%95%98%EA%B8%B0-1\",),\n",
    "\n",
    "    bs_kwargs=dict(\n",
    "        parse_only=bs4.SoupStrainer(class_=\"article_view\")  # 본문 텍스트만 추출\n",
    "    ),\n",
    ")\n",
    "docs = loader.load()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 19,
     "status": "ok",
     "timestamp": 1728648971016,
     "user": {
      "displayName": "이진규",
      "userId": "17496981203379941373"
     },
     "user_tz": -540
    },
    "id": "ElMJhCjdgjMN",
    "outputId": "523818f4-584a-4f12-9b5c-08c05d0b8d19"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(metadata={'source': 'https://mlrx.tistory.com/entry/%EC%B1%97%EB%B4%87-PDF-QA-%EC%B1%97%EB%B4%87-%EA%B0%9C%EB%B0%9C%ED%95%98%EA%B8%B0-1'}, page_content='\\n\\n\\n목적\\nPDF 문서 내용을 기반으로 질의응답(QA)를 할 수 있는 인트라넷에서 사용가능한 챗봇 개발\\n\\xa0\\n준비물\\npython\\nlangchain\\nopenai api key\\n\\xa0\\n과정\\n전체적인 플로우\\n\\n\\n1. PDF 에서 텍스트 추출하기\\nlangchain에서 제공하는 pdf loader를 이용해 pdf에서 text를 추출한다.\\nlangchain에서는 다양한 방법을 제공하므로 각자 상황에 맞는 방법을 사용하도록 한다.\\n(현재 글쓴이도 적절한 방법을 모색하고 있다.)\\nfrom langchain.document_loaders import UnstructuredPDFLoader\\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\\n\\nloader = UnstructuredPDFLoader(\\'../약관.pdf\\')\\ndata = loader.load()\\n\\nprint(f\"{len(data)}개의 문서를 가지고 있습니다.\")\\nprint(f\"문서에 {len(data[0].page_content)}개의 단어를 가지고 있습니다.\")\\n1개의 문서를 가지고 있습니다.\\n문서에 281012개의 단어를 가지고 있습니다.\\n\\xa0\\n2. 텍스트를 chunk로 나누기\\nembedding을 만들기 위해선 embedding이 모델이 소화할 수 있는 양만큼의 입력만 넣어줘야 한다. 그러기에 281012개의 단어를 가진 값을 통채로 넣어줄 수 없으므로 더 작은 단위로 묶어서 넣어주자.\\ntext_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\\ntexts = text_splitter.split_documents(data)\\nprint(f\"문서에 {len(texts)}개의 문서를 가지고 있습니다.\")\\n문서에 286개의 문서를 가지고 있습니다.\\n이 때 1000개씩 쪼겠기에 282개의 texts가 생성 될 것이라 기대되었으나 실제로는 286개의 texts가 생성되었다.\\xa0\\n실제로 texts의 원소들의 길이를 출력해보면\\xa0\\n983, 991, 952, 988, 979, 959, 944, 972...\\n와 같은데 그 이유는 RecursiveCharacterTextSplitter는 의미적으로 가장 연관성이 강한 텍스트 조각으로 보이는 문장, 단어를 가장 길게 유지하고자 하기 떄문이다.\\n\\xa0\\n3. Vector Store 만들기\\n이제 texts를 embedding으로 만들어 store를 구성하도록 하자.\\nvector store를 만드는 라이브러리는 다양하나(pinecone 등)\\xa0 여기선 Facebook에서 개발한 faiss를 사용한다.\\n이때 embedding 모델은 openai의 text-embedding-ada-002(default)를 사용한다.\\nfrom langchain.embeddings import OpenAIEmbeddings\\nfrom langchain.vectorstores import FAISS\\nimport faiss\\n\\nprint(f\\'예상되는 토큰 수 {num_tokens_from_string(data[0].page_content, \"cl100k_base\")}\\')\\n\\n# Create the vector store\\nstore = FAISS.from_texts([text.page_content for text in texts], OpenAIEmbeddings(openai_api_key=openai_api_key))\\n예상되는 토큰 수 258751\\nopenai api를 태우기전 예상 토큰 수를 계산해보고, 실제 태운 후 토큰 수를 확인해본다.\\n오전 7:30에서 볼 수 있는 실제 발생 토큰\\n\\n발생 비용\\n\\n\\n139개의 토큰 수가 차이 있음을 확인 할 수 있다.\\n\\xa0\\nimport pickle\\nfaiss.write_index(store.index, \"docs.index\")\\nwith open(\"faiss_store.pkl\", \"wb\") as f:\\n    pickle.dump(store, f)\\n\\xa0\\n이후 만들어진 값을 로컬 디스크에 저장하도록 한다.\\n\\xa0\\n4. 사용자 입력 받아 결과 생성하기\\nquery = \"보상하지 않는 손해에 대해 알려줘\"\\ndocs = store.similarity_search(query, k = 4)\\n\\xa0\\n위와 같이 보장하지 않는 손해에 대해 질문한다는 가정하에, 기 만든 vector store에서 가장 유사한 문서 4개를 추출해온다.\\n(docs를 출력하면 문서 내용이\\x08 출력되나 그 내용 중 일부만 공개한다.)\\n[Document(page_content=\\'위  ‘②  ◯1 에도  불구하고  다음의  손해는  보상하지  않습니다\\\\n\\\\n’\\\\n\\\\n.\\\\n\\\\n1)\\\\n\\\\n2)\\\\n\\\\n3)\\\\n\\\\n4)\\\\n\\\\n양도된  피보험자동차가  양수인  명의로  ...\\', metadata={}),\\n Document(page_content=\\')\\\\n\\\\n제 조 보상하지  않는  손해\\\\n\\\\n①\\\\n\\\\n8\\\\n\\\\n다음  중  어느  하나에  해당하는  손해는  대인배상 와  대물배상에서  보상하지  않습니다....\\', metadata={}),\\n Document(page_content=\\'도로교통법 에서  정한  사고발생  시의  조치를  하지  않은  경우를  말합니다 다만 주 정차된\\\\n\\\\n차만  손괴한  것이  분명한  경우에  피해자에게  인적사항을  제공하지  아니한  경우는\\', metadata={}),\\n Document(page_content=\\'손해배상보장법령에서  정한  금액을  한도...\\', metadata={})]\\n\\xa0\\n위 처럼 결과가 출력됨을 확인하였으므로 langchain의 load_qa_chain을 통해 문서와 query를 기반으로 GPT로 부터 결과를 얻어온다.\\nfrom langchain.chains.question_answering import load_qa_chain\\nfrom langchain import OpenAI\\nchain = load_qa_chain(OpenAI(model_name=\\'gpt-3.5-turbo\\', temperature=0, max_tokens=100, openai_api_key=openai_api_key), chain_type=\"map_reduce\")\\nresult_qa_chain = []\\nfor doc in docs:\\n    result_qa_chain.append(chain.run(input_documents=[doc], question=query))\\nresult_qa_chain\\n제대로 출력이 안됨을 볼 수 있다. 여기서 기 정의된 프롬프트가 포함되어 이는 load_qa_chain을 써서 그런 것으로 판단된다.\\n[\"I\\'m sorry, I cannot provide a final answer as the given content is not in a language that I am programmed to understand.\",\\n \\'대인배상 와 대물배상에서는 보상하지 않는 손해로, 보험계약자나 기명피보험자의 고의로 인한 손해와 기명피보험자 이외의 피보험자의 고의로 인한 손해가 있다.\\',\\n \\'There is no information provided about damages that are not covered by insurance.\\',\\n \"I\\'m sorry, I cannot provide a final answer as the given content is in Korean and I am not programmed to translate languages.\"]\\n\\xa0\\n앞으로 고심할 일은 다음과 같다.\\n\\n\\n위 문제를 하나하나 해결해나가며 글을 작성하도록 하겠다.\\n특히 인트라넷에서 사용 가능해야하므로 openai의 의존성은 완전히 없애서 완성도를 높여야 할 것이다. 또한, 비상업용 모델은 사용할 수 없으므로 상업적으로 사용가능한 dolly 로 대체할 것이다.\\n\\xa0\\n\\xa0\\n참고\\nhttps://www.youtube.com/watch?v=2xxziIWmaSA\\xa0\\nhttps://www.youtube.com/watch?v=h0DHDp1FbmQ\\xa0\\nhttps://www.youtube.com/watch?v=ih9PBGVVOO4\\xa0\\n\\n\\n\\n\\n공유하기\\n\\n게시글 관리\\n\\n\\n천천히찬찬히 \\n\\n\\n\\n')]\n",
      "<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "docs  # 웹에서 가져온 문서 객체 확인\n",
    "\n",
    "print(docs)  # 문서 내용 출력\n",
    "print(type(docs))  # 문서의 데이터 타입 확인 (문서 분할 전에 데이터 형태 점검)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YP5QhygYx85J"
   },
   "outputs": [],
   "source": [
    "# ✅ 2단계: 문서 텍스트 분할\n",
    "# 긴 문서를 일정 크기로 잘라주는 작업입니다. GPT가 한 번에 처리할 수 있는 분량으로 조각냅니다.\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "\n",
    "# 실제로 문서를 분할합니다. -> splits 에는 잘린 문장 블록들이 담깁니다.\n",
    "splits = text_splitter.split_documents(docs)\n",
    "splits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "or_2lgF84-1B"
   },
   "outputs": [],
   "source": [
    "splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "If0vTVB_x85J"
   },
   "outputs": [],
   "source": [
    "# ✅ 3단계: 텍스트를 임베딩(숫자 벡터)으로 변환하고 벡터 DB에 저장\n",
    "# 포인트: GPT가 문장을 이해할 수 있도록 숫자로 바꾸고, 저장소에 기억시키는 단계입니다.\n",
    "vectorstore = Chroma.from_documents(documents=splits, embedding=OpenAIEmbeddings())\n",
    "\n",
    "# 저장된 벡터로부터 검색 기능을 만드는 단계\n",
    "retriever = vectorstore.as_retriever()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fqHUgBJ6cD0o"
   },
   "outputs": [],
   "source": [
    "# ✅ 4단계: 질문을 던지고, 관련된 문서를 검색\n",
    "documents = retriever.get_relevant_documents(\"챗봇을 개발하기 위한 조건은?\")\n",
    "\n",
    "# 검색된 문서 출력\n",
    "for document in documents:\n",
    "    print(f\"문서 내용: {document.page_content}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "p5bWz7nbdf4y"
   },
   "outputs": [],
   "source": [
    "# ✅ 5단계: 기본 프롬프트 템플릿 정의\n",
    "# GPT가 응답을 만들 때 따라야 할 지침을 정해줍니다.\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"context\", \"question\"],\n",
    "    template=(\n",
    "        \"당신은 질문에 답하는 작업을 수행하는 보조자입니다. \"\n",
    "        \"다음에 제공된 문맥을 사용하여 질문에 답변하십시오. \"\n",
    "        \"답을 모르면 모른다고 말하십시오. \"\n",
    "        \"세 문장 이내로 답변하고 간결하게 작성하세요.\\n\"\n",
    "        \"질문: {question}\\n\"\n",
    "        \"문맥: {context}\\n\"\n",
    "        \"답변:\"\n",
    "    ),\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HUFUhMPYx85K"
   },
   "outputs": [],
   "source": [
    "\n",
    "# ✅ 6단계: Hub에서 고급 프롬프트 불러오기 (선택사항)\n",
    "# langchain hub에서 더 정교한 프롬프트를 불러옵니다.\n",
    "# https://smith.langchain.com/hub/rlm/rag-prompt\n",
    "# # https://smith.langchain.com/hub 에서 원하는 prompt를 받아올 수 있습니다.\n",
    "\n",
    "from langchain import hub\n",
    "\n",
    "# 프롬프트를 가져오기 위해 API 키 설\n",
    "# os.environ[\"LANGSMITH_API_KEY\"] = \n",
    "\n",
    "# 고급 프롬프트 가져오기 (더 정교한 지시를 포함)\n",
    "prompt = hub.pull(\"rlm/rag-prompt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1728648976245,
     "user": {
      "displayName": "이진규",
      "userId": "17496981203379941373"
     },
     "user_tz": -540
    },
    "id": "cT4N47h-hbVj",
    "outputId": "b84983af-7881-4254-e846-5383bccee6db"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_variables=['context', 'question'] input_types={} partial_variables={} metadata={'lc_hub_owner': 'rlm', 'lc_hub_repo': 'rag-prompt', 'lc_hub_commit_hash': '50442af133e61576e74536c6556cefe1fac147cad032f4377b60c436e6cdcb6e'} messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context', 'question'], input_types={}, partial_variables={}, template=\"You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\\nQuestion: {question} \\nContext: {context} \\nAnswer:\"), additional_kwargs={})]\n"
     ]
    }
   ],
   "source": [
    "# 프롬프트 내용 출력\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zdj7pMVwzMH8"
   },
   "outputs": [],
   "source": [
    "# ✅ 7단계: LLM 설정 및 체인 구성\n",
    "# 포인트: 질문과 문맥을 GPT에 함께 전달하고, 응답을 받는 전체 처리 흐름을 구성합니다.\n",
    "llm = ChatOpenAI(model_name=\"gpt-4o\", temperature=0.1)\n",
    "\n",
    "# 후처리 함수: 여러 문서를 하나의 문자열로 묶는 역할\n",
    "def format_docs(docs):\n",
    "    return \"\\n\".join(doc.page_content for doc in docs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2488,
     "status": "ok",
     "timestamp": 1728648979114,
     "user": {
      "displayName": "이진규",
      "userId": "17496981203379941373"
     },
     "user_tz": -540
    },
    "id": "nNjQkqqRx85K",
    "outputId": "f1bc1852-8dea-4e14-f822-031c923614f7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "벡터 임베딩을 만들기 위해서는 먼저 텍스트를 모델이 처리할 수 있는 크기로 나누어야 합니다. 그런 다음, OpenAI의 text-embedding-ada-002와 같은 임베딩 모델을 사용하여 텍스트를 벡터로 변환합니다. 변환된 벡터는 FAISS와 같은 라이브러리를 사용하여 벡터 스토어에 저장할 수 있습니다.\n"
     ]
    }
   ],
   "source": [
    "# 체인 구성\n",
    "rag_chain = (\n",
    "    {\"context\": retriever, \"question\": RunnablePassthrough()}  # 문맥을 검색하고 질문은 그대로 전달\n",
    "    | prompt  # 문맥 + 질문 → GPT로 전달될 형태로 포맷\n",
    "    | llm  # GPT가 응답 생성\n",
    "    | StrOutputParser()  # 출력된 응답을 사람이 읽을 수 있는 텍스트로 정리\n",
    ")\n",
    "\n",
    "# ✅ 8단계: 질문에 대해 응답 생성\n",
    "response = rag_chain.invoke(\"벡터 임베딩을 만들려면 어떻게 해야 하나요?\")\n",
    "print(response)  # GPT가 생성한 응답 출력"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
