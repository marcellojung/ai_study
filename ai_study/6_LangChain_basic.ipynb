{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ic9QsiyAoUCF"
      },
      "source": [
        "# LangChain 주요 모듈 및 사용법\n",
        "이 자료에서는 LangChain의 주요 모듈을 간단히 소개하고, LLM(대형 언어 모델) 호출과 관련된 기본적인 코드 사용법을 예제와 함께 설명합니다.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9iR_FvNynHVV"
      },
      "source": [
        "📘 LangChain, 어렵지 않습니다: 처음부터 쉽게 차근차근\n",
        "\n",
        "✨ LangChain은 무엇인가요?\n",
        "\n",
        "LangChain은 AI 언어모델을 활용한 다양한 기능을 쉽게 조립하고 확장할 수 있도록 도와주는 프레임워크입니다.\n",
        "쉽게 말해, **“AI 조립 키트”**입니다.\n",
        "\n",
        "🔧 LangChain을 왜 사용할까요?\n",
        "\n",
        "일반적인 AI 사용\tLangChain 사용\n",
        "\n",
        "단순 질문 → 답변만\t질문 → 요약 → 번역 → 저장 등 여러 단계 가능\n",
        "\n",
        "메모리 없음\t대화 기억 가능 (이전 질문 반영)\n",
        "\n",
        "기능 하나만 가능\t검색, 계산기 등 외부 도구 연결 가능\n",
        "\n",
        "코드 복잡함\t체인(chain) 방식으로 쉽게 조립\n",
        "\n",
        "🧱 LangChain의 큰 그림: 6가지 블록으로 이해하기\n",
        "\n",
        "LangChain을 이해하려면 이 6가지만 기억하세요:\n",
        "\n",
        "구성 요소\t하는 일\t비유\n",
        "\n",
        "1️⃣ LLM (언어모델)\tAI에게 질문하고 답 받기\t요리사\n",
        "\n",
        "2️⃣ Prompt (프롬프트)\t질문 틀 만들기\t요리 레시피\n",
        "\n",
        "3️⃣ Chain (체인)\t여러 작업 연결하기\t요리 순서 조립\n",
        "\n",
        "4️⃣ Output Parser\t결과를 정돈해서 받기\t예쁘게 포장하기\n",
        "\n",
        "5️⃣ Memory (메모리)\t이전 대화 기억하기\t메모장\n",
        "\n",
        "6️⃣ Agent & Tool\t외부 기능 사용하기 (검색 등)\t요리 도우미\n",
        "\n",
        "💡 예시로 쉽게 이해해볼까요?\n",
        "\n",
        "📦 LangChain은 AI 기능을 조립하는 블록 세트입니다.\n",
        "\n",
        "예를 들어, “AI에게 어떤 문서를 요약하고 → 영어로 번역하고 → 그 결과를 JSON 형식으로 저장”하고 싶다면?\n",
        "\n",
        "🎯 LangChain의 핵심 철학\n",
        "\n",
        "✅ AI를 단순히 ‘대답만 하는 모델’이 아닌,\n",
        "\n",
        "✅ 다양한 기능을 조합할 수 있는 스마트한 파트너로 만드는 것!\n",
        "\n",
        "🔁 LangChain이 진짜 좋은 이유는?\n",
        "\n",
        "\n",
        "코드 구조가 명확해서 배우기 쉽고,\n",
        "\n",
        "확장성이 뛰어나며,\n",
        "\n",
        "다양한 외부 도구와 연동할 수 있어서,\n",
        "단순 챗봇을 넘은 AI 서비스까지 만들 수 있습니다.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z9xoAul6n2tY"
      },
      "source": [
        "langchain은 필요한 도구들을 모아놓은 다이소와 비슷한 개념임"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FBWJ9gK7AlR0"
      },
      "source": [
        "1.모델 I/O\n",
        "\n",
        "2.데이터연결(RAG)\n",
        "\n",
        "3.체인(Chain)\n",
        "\n",
        "4.메모리\n",
        "\n",
        "5.에이전트/툴\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aU82XKtmzDpJ",
        "outputId": "a7e78a42-0499-4619-f175-3072504de397"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: openai in /usr/local/lib/python3.11/dist-packages (1.81.0)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from openai) (4.9.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from openai) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from openai) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from openai) (0.9.0)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from openai) (2.11.4)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.11/dist-packages (from openai) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.11/dist-packages (from openai) (4.13.2)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai) (2025.4.26)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.16.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->openai) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->openai) (0.4.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install langchain_community cohere openai langchain numexpr tabulate llmdantic langchain_openai -q\n",
        "!pip install --upgrade openai"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nmDVqJ09XcnQ"
      },
      "source": [
        "# 1.모델 I/O\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DY2KJWC7yioJ"
      },
      "source": [
        "## 언어모델 호출"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0_zEBrEpyoKu"
      },
      "source": [
        "### LLM 호출 방법: LangChain의 LLM 모듈을 활용하여 OpenAI의 언어 모델을 호출할 수 있습니다. 최근 버전에서는 OpenAI와 관련된 모듈이 별도의 라이브러리로 분리되어 있습니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ln7_JFE1ywXY"
      },
      "source": [
        "### 기본 LLM 생성 및 사용\n",
        "OpenAI의 `instruct` 모델을 사용하여 LLM 객체를 생성하고 간단한 명령을 수행할 수 있습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MZYfexCQnPqH",
        "outputId": "96fc74c2-b64c-44d7-e7a9-42a7da1216f8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "API 키 설정이 완료되었습니다.\n"
          ]
        }
      ],
      "source": [
        "# 포인트: 언어모델과 연결해서 실제 응답을 받아보는 핵심 단계입니다.\n",
        "\n",
        "# 1. 필수 라이브러리 가져오기 및 API 키 설정\n",
        "import openai  # OpenAI에서 제공하는 공식 Python 라이브러리 → 모델 호출에 필요\n",
        "import os      # 운영체제와 관련된 기능을 제공 (환경변수 설정 등)\n",
        "\n",
        "# 2. API 키 설정\n",
        "# 포인트: 보안상 중요한 정보이므로, 직접 노출하지 않고 환경변수로 설정합니다.\n",
        "# API 키는 OpenAI 계정에서 발급받은 인증 토큰입니다. 이 키를 통해 내 계정으로 모델을 사용할 수 있습니다.\n",
        "# os.environ['OPENAI_API_KEY'] \n",
        "# 3. OpenAI 클라이언트 객체 생성\n",
        "# 포인트: 이 client 객체를 통해 모델 호출 등 다양한 기능을 사용할 수 있습니다.\n",
        "client = openai.OpenAI()\n",
        "print(\"API 키 설정이 완료되었습니다.\")  # 키 설정이 완료되었음을 출력"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "qpAA7_YcoTpn"
      },
      "outputs": [],
      "source": [
        "# 4. LangChain 라이브러리에서 OpenAI 모듈 가져오기\n",
        "# LangChain은 OpenAI 모델을 포함한 여러 언어모델을 쉽게 연결하고 사용할 수 있게 도와주는 프레임워크입니다.\n",
        "from langchain_openai import OpenAI\n",
        "\n",
        "# 5. LLM(언어 모델) 객체 생성\n",
        "# model: 사용할 모델 지정 → gpt-3.5-turbo-instruct는 '명령어 기반' 응답에 적합\n",
        "# temperature: 창의성 설정 (0.5면 중간 수준의 창의성)\n",
        "# max_tokens: 응답의 최대 길이 (토큰 기준)\n",
        "llm = OpenAI(model='gpt-3.5-turbo-instruct', temperature=0.5, max_tokens=500)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ya3NqAP6PvE5",
        "outputId": "d1a59f31-20d0-4be6-a0f1-3b6201e57e54"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "한국에서 유명한 장소 중 하나는 경복궁입니다. 경복궁은 조선 왕조의 궁궐로서, 서울의 중심지인 종로구에 위치하고 있으며, 우리나라의 대표적인 궁궐 중 하나로 국보 1호로 지정되어 있습니다. 매년 많은 관광객들이 찾는 곳으로, 아름다운 조선 시대의 건축물과 정원을 감상할 수 있으며, 궁궐 내부에는 국립민속박물관과 궁중문화전당 등 다양한 문화 시설들이 있어서 한국의 전통문화를 체험할 수 있습니다. 또한, 매일 오후에는 궁궐 내부에서 왕궁 수호병의 교대식이 진행되는 등 다양한 이벤트와 체험 프로그램도 제공되어 관광객들에게 인기를 끌고 있습니다. \n"
          ]
        }
      ],
      "source": [
        "# 6. LLM 모델에 명령어 전달 및 응답 받기\n",
        "# invoke(): 입력된 문장을 모델에게 전달하고, 그 결과(답변)를 받아오는 메서드\n",
        "# 예: '한국에서 유명한 장소 하나를 알려주세요.' → 모델이 그에 대한 답변 생성\n",
        "response = llm.invoke(\"한국에서 유명한 장소하나를 알려주세요.\")\n",
        "\n",
        "# 7. 결과 출력\n",
        "print(response)  # 콘솔에 모델이 생성한 응답을 출력\n",
        "\n",
        "# 포인트: invoke는 Chat 모델을 사용할 때 사용자 역할(user)을 명시하지 않아도 자동으로 처리됩니다.\n",
        "# 자연스러운 대화처럼 질문만 넣으면 AI가 적절한 답변을 생성합니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ve_EW5SHosed"
      },
      "source": [
        "### Chat LLM 사용 방법\n",
        "\n",
        "대화형 LLM을 사용하기 위해 ChatOpenAI 클래스를 사용할 수 있습니다. 이 예제에서는 gpt-4o 모델을 사용합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ORtVF1xNohDb",
        "outputId": "dbf819c4-0cb0-40f8-9e25-c9fbe73c64c1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "content='한국에서 가장 인기 있는 장소 중 하나는 서울의 경복궁입니다. 경복궁은 조선 왕조의 법궁으로, 1395년에 건립되었습니다. 이 궁궐은 한국 전통 건축의 아름다움을 잘 보여주며, 역사적으로도 중요한 의미를 지니고 있습니다.\\n\\n경복궁은 광화문, 근정전, 경회루 등 여러 아름다운 건축물로 구성되어 있습니다. 특히 근정전은 국왕이 정무를 보던 중심 건물로, 그 웅장한 규모와 섬세한 장식이 인상적입니다. 경회루는 연못 위에 세워진 누각으로, 예전에는 연회나 중요한 회의를 열던 장소였습니다.\\n\\n또한, 경복궁은 사계절마다 다른 매력을 선사합니다. 봄에는 벚꽃과 함께 아름다운 풍경을 즐길 수 있고, 가을에는 단풍이 궁궐을 물들입니다. 이러한 이유로 경복궁은 많은 관광객과 현지인들이 찾는 명소입니다. 또한, 경복궁에서는 전통 의상인 한복을 입고 궁궐을 둘러보는 체험도 할 수 있어, 한국의 전통 문화를 직접 경험할 수 있는 기회를 제공합니다.' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 280, 'prompt_tokens': 22, 'total_tokens': 302, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_76544d79cb', 'id': 'chatcmpl-BZo0cMmVTtzMtr8GaZNYC888IsT2W', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None} id='run--ad6285fd-5248-428c-a1a8-f588fdfc3610-0' usage_metadata={'input_tokens': 22, 'output_tokens': 280, 'total_tokens': 302, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n"
          ]
        }
      ],
      "source": [
        "from langchain_openai import ChatOpenAI  # LangChain 라이브러리에서 ChatOpenAI 가져오기\n",
        "# Chat LLM(언어 모델) 객체 생성\n",
        "chat_llm = ChatOpenAI(model='gpt-4o', temperature=0.5, max_tokens=1024)\n",
        "# 사용자 질문에 대한 응답 요청\n",
        "question = \"한국에서 가장 인기있는 장소의 그 장소에 대해 설명해 주세요.\"\n",
        "# invoke 메서드로 질문에 대한 답변 생성\n",
        "response = chat_llm.invoke(question)\n",
        "# 생성된 답변 출력\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cfV8yltyXTYv",
        "outputId": "2388fe41-84d5-4c06-fc13-d7f9c3784107"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "한국에서 가장 인기 있는 장소 중 하나는 서울의 경복궁입니다. 경복궁은 조선 왕조의 법궁으로, 1395년에 건립되었습니다. 이 궁궐은 한국 전통 건축의 아름다움을 잘 보여주며, 역사적으로도 중요한 의미를 지니고 있습니다.\n",
            "\n",
            "경복궁은 광화문, 근정전, 경회루 등 여러 아름다운 건축물로 구성되어 있습니다. 특히 근정전은 국왕이 정무를 보던 중심 건물로, 그 웅장한 규모와 섬세한 장식이 인상적입니다. 경회루는 연못 위에 세워진 누각으로, 예전에는 연회나 중요한 회의를 열던 장소였습니다.\n",
            "\n",
            "또한, 경복궁은 사계절마다 다른 매력을 선사합니다. 봄에는 벚꽃과 함께 아름다운 풍경을 즐길 수 있고, 가을에는 단풍이 궁궐을 물들입니다. 이러한 이유로 경복궁은 많은 관광객과 현지인들이 찾는 명소입니다. 또한, 경복궁에서는 전통 의상인 한복을 입고 궁궐을 둘러보는 체험도 할 수 있어, 한국의 전통 문화를 직접 경험할 수 있는 기회를 제공합니다.\n"
          ]
        }
      ],
      "source": [
        "print(response.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o_UD8Enup5jK"
      },
      "source": [
        "## 1.2프롬프트 템플릿(Prompt Template)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lkCynGPuqEde"
      },
      "source": [
        "LLM(대형 언어 모델) 어플리케이션에서 프롬프트는 사용자의 입력을 적절하게 변형하여 모델에 전달됩니다. 이를 위해 LangChain 라이브러리의 `PromptTemplate`과 `ChatPromptTemplate`을 사용할 수 있습니다. 이 코드에서는 이러한 템플릿을 이용해 간단한 프롬프트를 생성하는 방법을 설명합니다.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZQbL7Umm_BwH"
      },
      "source": [
        "### 1.2.1 기본적인 프롬프트 템플릿 생성\n",
        "\n",
        " 사용자가 제공한 입력을 바탕으로 간단한 프롬프트를 생성"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "r902ErNcqTDu"
      },
      "outputs": [],
      "source": [
        "# 필요한 라이브러리 임포트\n",
        "from langchain.prompts import PromptTemplate  # 프롬프트 템플릿 생성을 위한 기본 라이브러리\n",
        "from langchain.prompts import ChatPromptTemplate  # 챗봇용 프롬프트 템플릿 생성을 위한 라이브러리\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0aAlhgeEp8i5",
        "outputId": "c38cf6c7-c110-4b8a-d0cf-7141e1527d71"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "당신은 초등학생에게 {term}을(를) 쉽게 설명하는 챗봇입니다.\n",
            "당신은 초등학생에게 인공지능을(를) 쉽게 설명하는 챗봇입니다.\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# 초등학생에게 용어를 쉽게 설명하는 챗봇 템플릿 문자열을 정의\n",
        "simple_explain_template = \"당신은 초등학생에게 {term}을(를) 쉽게 설명하는 챗봇입니다.\"\n",
        "print(simple_explain_template)  # 템플릿이 제대로 작성되었는지 출력해 확인\n",
        "\n",
        "# 기본 파이썬에서 제공하는 .format() 함수로 템플릿을 활용하는 방법\n",
        "# 'term'이라는 자리 표시자에 '인공지능'이라는 값을 넣어 템플릿을 포맷\n",
        "formatted_template = simple_explain_template.format(term='인공지능')\n",
        "print(formatted_template)  # 포맷된 템플릿을 출력해 확인\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q5AS9pokn2pg",
        "outputId": "a5306488-4c59-4674-e092-b5d0134272a3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "안녕하세요! 저는 초등학생에게 인공지능을 쉽게 설명하는 챗봇입니다. 인공지능이란 무엇일까요? 인공지능은 사람처럼 생각하고 학습할 수 있는 컴퓨터 시스템을 말해요. 이 컴퓨터 시스템은 우리가 일상 생활에서 사용하는 스마트폰, 컴퓨터, 로봇 등에서도 사용되고 있어요. 인공지능은 우리가 입력한 정보를 분석하고 학습하여 우리가 원하는 답변이나 결과를 제공해줄 수 있어요. 예를 들어, 우리가 스마트폰에 음성으로 \"음악을 틀어줘\"라고 말하면, 인공지능이 음악 앱을 실행시켜줄 수 있어요! 이렇게 인공지능은 우리의 생활을 더 편리하고 즐겁게 만들어줄 수 있답니다. 이렇게 쉽게 설명해볼 수 있었나요? 더 궁금한 점이 있으면 언제든지 물어보세요!\n"
          ]
        }
      ],
      "source": [
        "# LangChain의 PromptTemplate을 사용하여 프롬프트를 생성\n",
        "# input_variables: 템플릿에서 사용할 변수를 정의 (여기서는 'term'을 사용)\n",
        "# template: 미리 정의한 챗봇 설명 템플릿을 연결\n",
        "explain_prompt = PromptTemplate(input_variables=[\"term\"],\n",
        "                                template=simple_explain_template)\n",
        "# PromptTemplate을 이용해 'term' 변수에 '인공지능'을 넣어 템플릿을 포맷\n",
        "formatted_prompt = explain_prompt.format(term=\"인공지능\")\n",
        "# LLM 모델을 호출해 포맷된 템플릿을 바탕으로 응답 생성 (실제 예시)\n",
        "response = llm.invoke(formatted_prompt)  # LLM을 이용해 응답을 생성하는 부분\n",
        "print(response)  # 생성된 응답을 출력\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "66Ked0lqQVfN",
        "outputId": "94397980-c2b6-4259-888f-03b9409b373c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "아반떼는 어느 회사에서 개발한 제품인가요？\n",
            "소나타는 어느 회사에서 개발한 제품인가요？\n",
            "벤츠는 어느 회사에서 개발한 제품인가요？\n"
          ]
        }
      ],
      "source": [
        "# 추가 예시: PromptTemplate을 이용해 제품명을 넣어 프롬프트 생성\n",
        "from langchain import PromptTemplate\n",
        "\n",
        "# 특정 템플릿 문자열을 사용하여 PromptTemplate 인스턴스 생성\n",
        "# 'product'라는 자리 표시자가 포함된 템플릿 문자열 정의\n",
        "prompt = PromptTemplate(\n",
        "    template=\"{product}는 어느 회사에서 개발한 제품인가요？\",\n",
        "    input_variables=[\"product\"]  # 템플릿에서 사용할 변수 'product'를 정의\n",
        ")\n",
        "\n",
        "# 'prompt' 객체를 사용해 다양한 제품명으로 템플릿을 포맷한 후 결과를 출력\n",
        "print(prompt.format(product=\"아반떼\"))  # '아반떼'로 템플릿 포맷 후 출력\n",
        "print(prompt.format(product=\"소나타\"))  # '소나타'로 템플릿 포맷 후 출력\n",
        "print(prompt.format(product=\"벤츠\"))  # '벤츠'로 템플릿 포맷 후 출력\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X-zSeeIFqVjs"
      },
      "source": [
        "### 1.2.2. 두 개의 매개변수를 사용하는 프롬프트 생성\n",
        "\n",
        "여러 매개변수를 받는 프롬프트: 특정 주제에 대해 원하는 언어로 설명하는 프롬프트를 생성\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "lOBQOQQSqP_T"
      },
      "outputs": [],
      "source": [
        "# 번역을 위한 프롬프트 템플릿\n",
        "translate_template = \"{topic}에 대해 {language}로 설명해 주세요.\"\n",
        "\n",
        "# ✨ 포인트 설명:\n",
        "# 프롬프트 템플릿이란?\n",
        "# 👉 프롬프트는 AI에게 질문할 \"문장 틀\"을 미리 만들어두는 것입니다.\n",
        "# 위의 템플릿은 사용자가 '주제(topic)'와 '언어(language)'를 넣으면,\n",
        "# 그걸 바탕으로 \"무엇에 대해, 어떤 언어로 설명해 달라\"는 문장을 자동으로 만들어줍니다.\n",
        "# 예: topic='컴퓨터 비전', language='한국어' → \"컴퓨터 비전에 대해 한국어로 설명해 주세요.\"\n",
        "\n",
        "# PromptTemplate으로 템플릿 생성\n",
        "translate_prompt = PromptTemplate(input_variables=[\"topic\", \"language\"],\n",
        "                                  template=translate_template)\n",
        "\n",
        "# ✨ 포인트 설명:\n",
        "# PromptTemplate 클래스는 LangChain에서 제공하는 기능으로,\n",
        "# 위에서 만든 템플릿 문자열을 바탕으로 실제로 사용할 \"프롬프트 객체\"를 만들어줍니다.\n",
        "# - input_variables: 템플릿 안에 채워 넣을 변수 이름들을 리스트로 명시합니다.\n",
        "# 즉, 사용자는 topic과 language 두 가지 값을 넣어야 합니다.\n",
        "\n",
        "# 프롬프트 생성 및 실행 예시\n",
        "formatted_translate_prompt = translate_prompt.format(topic='컴퓨터 비전', language='한국어')\n",
        "\n",
        "# ✨ 포인트 설명:\n",
        "# .format() 메서드를 통해 실제 값을 템플릿에 집어넣습니다.\n",
        "# 여기서는 topic='컴퓨터 비전', language='한국어'를 넣은 결과\n",
        "# 최종 프롬프트는 이렇게 됩니다 → \"컴퓨터 비전에 대해 한국어로 설명해 주세요.\"\n",
        "\n",
        "response_translate = llm.invoke(formatted_translate_prompt)\n",
        "\n",
        "# ✨ 포인트 설명:\n",
        "# llm.invoke()는 AI 모델(GPT 등)에게 프롬프트를 전달해 답변을 요청하는 함수입니다.\n",
        "# 즉, 앞에서 만든 문장을 AI에게 그대로 물어보고, 그에 대한 답변을 받아오는 단계입니다.\n",
        "\n",
        "# 💡 요약 흐름 예시:\n",
        "# 1. 틀 만들기: \"{topic}에 대해 {language}로 설명해 주세요.\"\n",
        "# 2. 값 넣기: topic=\"컴퓨터 비전\", language=\"한국어\"\n",
        "# 3. 완성된 질문: \"컴퓨터 비전에 대해 한국어로 설명해 주세요.\"\n",
        "# 4. AI에게 전달 → 답변 받아오기 (예: \"컴퓨터 비전은 ... 입니다.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hiuhysk9QbMP",
        "outputId": "c11dc38d-20fd-4393-a73b-a9053fd5cc24"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "컴퓨터 비전은 컴퓨터가 인간의 시각 능력을 모방하여 디지털 이미지나 비디오를 처리하고 이해하는 분야입니다. 이를 위해 컴퓨터 비전은 이미지나 비디오에서 모양, 색상, 패턴 등의 특징을 추출하고 이를 분석하여 객체를 인식하고 분류하는 기술을 사용합니다. 또한, 컴퓨터 비전은 이미지나 비디오에서 원하는 정보를 추출하고 이를 활용하여 다양한 응용 분야에서 사용될 수 있습니다. 예를 들어, 자율주행차의 경우 컴퓨터 비전 기술을 사용하여 주변 환경을 인식하고 이를 바탕으로 운전을 제어합니다. 또한, 의료 분야에서는 컴퓨터 비전을 사용하여 X-ray나 MRI 이미지를 분석하여 질병을 진단하거나 암 조직을 탐지하는 등의 의료 영상 분석에 활용될 수 있습니다. 컴퓨터 비전은 인간의 시각 능력을 대신하여 다양한 분야에서 활용되고 있으며, 더 나은 인공지능 기술을 발전시키는 데에도 중요한 역할을 합니다.\n"
          ]
        }
      ],
      "source": [
        "print(response_translate)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J2nVT7aYYMA3",
        "outputId": "5566dfd2-b290-4b66-a9ed-5c370b95b8ac"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "content='축구선수는 축구라는 스포츠에서 경기를 하는 선수로, 팀의 일원으로서 공격, 수비, 미드필더 등 다양한 포지션에서 활약합니다. 이들은 뛰어난 체력과 기술, 전술 이해도를 바탕으로 경기를 진행하며, 팀의 승리를 위해 협력합니다. 또한, 축구선수는 종종 팬들에게 영감을 주고, 사회적 영향력을 행사하는 역할도 수행합니다.' additional_kwargs={} response_metadata={'token_usage': {'completion_tokens': 98, 'prompt_tokens': 20, 'total_tokens': 118, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini', 'system_fingerprint': 'fp_54eb4bd693', 'finish_reason': 'stop', 'logprobs': None} id='run--d15702ad-ee7a-4c55-8c64-50cc6c5cf65f-0'\n"
          ]
        }
      ],
      "source": [
        "translate_template = \"한국의 {topic}에 대한 {language}명의 선수를 말해줘! 쉼표로 구분된 목록을 만들어줘\"\n",
        "formatted_translate_prompt = translate_prompt.format(topic='축구선수', language='3')\n",
        "response_translate = llm.invoke(formatted_translate_prompt)\n",
        "print(response_translate)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8BCbC8qTYFPr",
        "outputId": "d633a495-51b7-4a3f-9280-278ee04e58e7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "안녕하세요, 저는 김철수입니다. 제가 선호하는 과목은 수학입니다.\n"
          ]
        }
      ],
      "source": [
        "from langchain_core.prompts import PromptTemplate\n",
        "\n",
        "# 템플릿 정의: {name}과 {subject}라는 변수를 포함\n",
        "template_text = \"안녕하세요, 저는 {name}입니다. 제가 선호하는 과목은 {subject}입니다.\"\n",
        "\n",
        "# PromptTemplate 인스턴스 생성\n",
        "prompt_template = PromptTemplate.from_template(template_text)\n",
        "\n",
        "# 템플릿에 값을 채워서 완성된 프롬프트 생성\n",
        "filled_prompt = prompt_template.format(name=\"김철수\", subject=\"수학\")\n",
        "\n",
        "# 완성된 프롬프트 출력\n",
        "print(filled_prompt)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aAgfIg2xqZ_-"
      },
      "source": [
        "### 1.2.3. 채팅 메시지를 위한 ChatPromptTemplate 생성\n",
        "\n",
        "채팅 대화의 흐름을 설정하여 프롬프트를 생성합니다. 시스템 메시지와 사용자 메시지를 순차적으로 연결해 자연스러운 대화를 유도\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l5UxqMzUqRH9",
        "outputId": "8406d495-7f75-4e49-bf0f-ba1c221ba03f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "System: LangChain은 다양한 언어를 배우는 데 도움이 될 뿐만 아니라, 문장의 구조와 문법을 이해하는 데도 도움이 됩니다. 또한 다양한 문화와 관련된 지식도 습득할 수 있어서 넓은 시야를 갖게 될 수 있습니다.\n"
          ]
        }
      ],
      "source": [
        "# ChatPromptTemplate을 이용해 채팅 대화 흐름 정의\n",
        "chat_prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", '당신은 모든 질문에 대해 긍정적인 답변을 하는 챗봇입니다.'),  # 시스템 메시지: 챗봇의 역할과 성격을 정의\n",
        "    (\"user\", 'LangChain을 배우면 어떤 유용한 점이 있나요?')  # 사용자 메시지: 질문 내용\n",
        "])\n",
        "\n",
        "# 메시지를 포맷팅하여 LLM에 전달\n",
        "formatted_chat_messages = chat_prompt.format_messages()  # 대화 내용을 LLM이 이해할 수 있는 형식으로 정리\n",
        "response_chat = llm.invoke(formatted_chat_messages)  # 정리된 메시지를 LLM에 전달하여 응답 생성\n",
        "print(response_chat)  # 응답 출력\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZedwKFNxsd72",
        "outputId": "2091ff12-4a60-42b1-b106-92b182af4621"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "안녕하세요, 이짜이밍님. 경제에 관심이 있으시다니 멋진 일이네요! 경제는 매우 중요한 분야이며 다양한 직업 기회를 제공해줍니다. 이와 관련하여 몇 가지 조언을 드릴게요.\n",
            "\n",
            "1. 교육: 경제 분야에서 경쟁력을 갖추기 위해서는 적합한 교육이 필요합니다. 대학이나 전문 학교에서 경제학 전공을 선택하거나 관련 분야의 석사 또는 박사 학위를 취득하는 것이 도움이 될 수 있습니다.\n",
            "\n",
            "2. 경제 이해: 경제에 대한 기본 지식을 탄탄히 쌓는 것이 중요합니다. 경제 이론과 실무 지식을 꾸준히 학습하고 최신 경제 동향을 파악하는 습관을 가지세요.\n",
            "\n",
            "3. 경제 관련 경험: 경제 분야에서의 경험이 중요합니다. 경제 연구원, 금융 기관, 정부 기관 등에서 인턴십이나 경제 관련 일자리를 통해 실무 경험을 쌓는 것이 도움이 될 수 있습니다.\n",
            "\n",
            "4. 네트워킹: 경제 분야에서 성공을 거두기 위해서는 좋은 인맥이 중요합니다. 세미나나 컨퍼런스에 참석하거나 경제학자들과 소통할 수 있는 기회를 찾아 인적 네트워크를 구축하세요.\n",
            "\n",
            "많은 노력과 열정이 필요하지만, 경제 분야에서 성공을 거두기 위해 꾸준한 노력을 기울이시면 좋은 결과를 얻을 수 있을 것입니다. 행운을 빕니다!\n"
          ]
        }
      ],
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "\n",
        "# 1. 템플릿 메시지 정의: system과 user 메시지에 변수 포함\n",
        "chat_prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"당신은 친절한 조언을 해주는 교사입니다.\"),\n",
        "    (\"user\", \"안녕하세요, 저는 {name}입니다. 제가 선호하는 과목은 {subject}입니다. 저에게 어떤 조언을 해주실 수 있나요?\")\n",
        "])\n",
        "\n",
        "# 2. 변수값 채워넣기\n",
        "filled_messages = chat_prompt.format_messages(\n",
        "    name=\"이짜이밍\",\n",
        "    subject=\"경제\"\n",
        ")\n",
        "\n",
        "# 3. 메시지를 LLM에 전달 (예: ChatOpenAI 등)\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "llm = ChatOpenAI()\n",
        "response = llm.invoke(filled_messages)\n",
        "\n",
        "# 4. 결과 출력\n",
        "print(response.content)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KlJTvBvZ3NcE"
      },
      "source": [
        "## 출력형식(Output Parser)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156
        },
        "id": "H1LNNX_lZfSy",
        "outputId": "30c2c0b8-0e38-4a84-b55f-751ee991254a"
      },
      "outputs": [
        {
          "ename": "TypeError",
          "evalue": "'module' object is not callable",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-57-a87ab6d1ae8f>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlangchain\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mlangchain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchat_models\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m: 'module' object is not callable"
          ]
        }
      ],
      "source": [
        "import langchain\n",
        "langchain.chat_model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4apA97KQ3Q17",
        "outputId": "0ac352c3-ed29-476d-a66d-edcbd9a965c5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['이재명', '윤석열', '박지현']\n"
          ]
        }
      ],
      "source": [
        "# 🧠 포인트: LangChain을 이용해 LLM에게 질문하고, 그 결과를 '쉼표로 구분된 리스트' 형태로 받아오는 구조입니다.\n",
        "\n",
        "# ✅ 필요한 모듈 불러오기: LLM 모델, 프롬프트 템플릿, 출력 형식 파서를 가져옵니다.\n",
        "from langchain.output_parsers import CommaSeparatedListOutputParser  # → 결과를 쉼표(,)로 구분된 리스트로 변환해주는 파서\n",
        "from langchain.prompts import PromptTemplate  # → 프롬프트를 템플릿 형식으로 구조화할 수 있게 도와주는 도구\n",
        "from langchain.chat_models import ChatOpenAI  # → OpenAI의 챗 기반 모델을 LangChain에서 사용할 수 있게 해주는 모듈\n",
        "\n",
        "# ✅ LLM(언어모델) 객체 초기화: GPT-4 모델을 사용해서 답변을 생성합니다.\n",
        "llm = ChatOpenAI(\n",
        "    temperature=0,         # 🎯 포인트: temperature는 창의성(무작위성)의 정도 → 0이면 가장 정적인/일관된 답변 생성\n",
        "    max_tokens=2048,       # 응답의 최대 길이를 설정 (토큰 기준)\n",
        "    model_name='gpt-4o-mini',    # 사용할 LLM 모델 이름 지정\n",
        ")\n",
        "\n",
        "# ✅ 출력 형식 설정: 쉼표로 구분된 문자열로 응답을 받고자 할 때 사용하는 파서\n",
        "output_parser = CommaSeparatedListOutputParser()  # → 예: \"손흥민, 황희찬, 김민재\"\n",
        "format_instructions = output_parser.get_format_instructions()\n",
        "# → LLM에게 \"답변은 쉼표로 구분된 리스트 형식으로 해줘\" 라고 알려주는 역할\n",
        "\n",
        "# ✅ 프롬프트 템플릿 정의: 질문 문장을 만드는 틀을 미리 정의해둡니다.\n",
        "prompt = PromptTemplate(\n",
        "    # 🔧 template: 프롬프트 형식 문자열 정의\n",
        "    # {subject} → 유동적으로 바뀌는 질문 주제 (예: \"한국의 축구선수\")\n",
        "    # {format_instructions} → 출력 형식 가이드라인 삽입 (위에서 가져온 쉼표 지침)\n",
        "    template=\"{subject}에 대한 3명의 선수를 말해줘!.\\n{format_instructions}\",\n",
        "\n",
        "    # 📌 input_variables: 위 템플릿에서 어떤 변수들을 사용자 입력으로 받을 건지 정의\n",
        "    input_variables=[\"subject\"],\n",
        "\n",
        "    # 📌 partial_variables: 고정적으로 넣어줄 값 (여기선 format_instructions를 항상 넣어줌)\n",
        "    partial_variables={\"format_instructions\": format_instructions}\n",
        ")\n",
        "\n",
        "# ✅ 실제 사용할 질문 내용 정의\n",
        "query = \"한국의 정치인\"\n",
        "# → 예: 템플릿에서 {subject} 자리에 들어가게 됨\n",
        "\n",
        "# ✅ 프롬프트를 모델에게 전달하여 응답 생성\n",
        "# prompt.format(subject=query) → \"한국의 축구선수에 대한 3명의 선수를 말해줘! 쉼표로 구분된 목록을 만들어줘.\"\n",
        "# 📌 LLM이 이해할 수 있는 형태의 문장으로 변환한 뒤, 모델에 전달\n",
        "output = llm.predict(text=prompt.format(subject=query))\n",
        "\n",
        "# ✅ 모델의 응답을 파서를 이용해 파싱 (즉, 쉼표 기준으로 쪼개서 리스트로 변환)\n",
        "parsed_result = output_parser.parse(output)\n",
        "\n",
        "# ✅ 최종 결과 출력\n",
        "# 예: ['손흥민', '황희찬', '김민재']\n",
        "print(parsed_result)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CBCNZocI4ixP",
        "outputId": "2336c9e4-1f9f-4ebd-878f-fa907c4b42e3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'Korean_Baseball_Players': [{'name': '이종범', 'position': '외야수', 'team': 'KIA 타이거즈'}, {'name': '박찬호', 'position': '투수', 'team': '한화 이글스'}, {'name': '김광현', 'position': '투수', 'team': 'SSG 랜더스'}, {'name': '최정', 'position': '내야수', 'team': 'SSG 랜더스'}, {'name': '이대호', 'position': '내야수', 'team': '롯데 자이언츠'}]}\n"
          ]
        }
      ],
      "source": [
        "# ✅ 필요한 모듈 불러오기: LangChain에서 제공하는 주요 기능들을 사용하기 위한 준비입니다.\n",
        "from langchain_core.output_parsers import JsonOutputParser  # → 모델의 응답을 JSON 형식으로 변환해주는 파서\n",
        "from langchain.prompts import PromptTemplate               # → 프롬프트(질문)을 템플릿 형식으로 구성하는 도구\n",
        "from langchain.chat_models import ChatOpenAI               # → OpenAI의 챗 모델(GPT 시리즈)을 사용하는 클래스\n",
        "# ✅ 1단계: 언어모델(LLM) 초기화\n",
        "llm = ChatOpenAI(\n",
        "    temperature=0,        # 🎯 포인트: temperature는 '창의성'을 조절 → 0이면 가장 일관되고 안정적인 응답 생성\n",
        "    max_tokens=2048,      # → 모델이 생성할 수 있는 응답의 최대 길이(토큰 수 기준)\n",
        "    model_name='gpt-4o-mini',   # → 사용할 LLM 모델 이름 설정 (여기선 GPT-4 사용)\n",
        ")\n",
        "\n",
        "# ✅ 2단계: 출력 형식 지정 - JSON 형식으로 결과를 받기 위한 파서 초기화\n",
        "output_parser = JsonOutputParser()  # → 모델이 생성한 텍스트를 JSON 구조로 파싱해줌 (예: {\"선수\": [\"손흥민\", \"황희찬\", \"김민재\"]})\n",
        "\n",
        "# → 모델에게 \"답변은 JSON 형태로 해줘\"라고 알려주는 설명 텍스트를 가져옴\n",
        "format_instructions = output_parser.get_format_instructions()\n",
        "# 예: \"응답은 다음 JSON 형식으로 작성하세요: {\"선수\": [항목1, 항목2, 항목3]}\"\n",
        "\n",
        "# ✅ 3단계: 프롬프트 템플릿 구성\n",
        "prompt = PromptTemplate(\n",
        "    # 템플릿 본문: 질문 문장과 출력 형식 안내를 포함\n",
        "    # {subject} → 사용자 입력으로 바뀌는 질문 내용\n",
        "    # {format_instructions} → 출력 형식 지시문 (항상 JSON 형태로 응답하라고 명시)\n",
        "    template=\"5명의 선수를 말해줘!{subject}.\\n{format_instructions}\",\n",
        "\n",
        "    # 입력 변수 정의 → 템플릿에서 바뀌는 부분 지정\n",
        "    input_variables=[\"subject\"],\n",
        "\n",
        "    # 고정 변수 정의 → format_instructions는 항상 고정되어 들어감\n",
        "    partial_variables={\"format_instructions\": format_instructions}\n",
        ")\n",
        "\n",
        "# ✅ 4단계: 실제 사용할 질문 정의\n",
        "query = \"한국의 야구선수는?\"\n",
        "# → {subject} 자리에 들어갈 사용자 입력\n",
        "\n",
        "# ✅ 5단계: 완성된 프롬프트를 모델에 전달하여 응답 생성\n",
        "output = llm.predict(text=prompt.format(subject=query))\n",
        "# 예: \"3명의 선수를 말해줘! 한국의 축구선수는?.\\n응답은 JSON 형식으로 작성하세요: {\"선수\": [...]}\"\n",
        "\n",
        "# ✅ 6단계: 모델이 출력한 결과를 파서로 변환 (→ JSON 형식의 딕셔너리 형태로 가공)\n",
        "parsed_result = output_parser.parse(output)\n",
        "# 예: {\"선수\": [\"손흥민\", \"황희찬\", \"김민재\"]}\n",
        "\n",
        "# ✅ 7단계: 최종 결과 출력\n",
        "print(parsed_result)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "id": "AUWOQ6oiXqid"
      },
      "outputs": [],
      "source": [
        "from langchain.output_parsers import CommaSeparatedListOutputParser\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "\n",
        "# [포인트] LLM(Language Model) 객체 초기화\n",
        "# → 우리가 사용할 GPT-4 모델을 불러오는 부분입니다.\n",
        "llm = ChatOpenAI(\n",
        "    temperature=0,        # [포인트] 창의성(무작위성) 설정: 0은 가장 논리적이고 일관된 응답을 의미합니다.\n",
        "    max_tokens=2048,      # 생성할 응답의 최대 길이를 설정 (문장의 분량을 제한하는 역할)\n",
        "    model_name='gpt-4',   # 사용할 오픈AI의 모델 이름 지정 (gpt-3.5도 사용 가능)\n",
        ")\n",
        "\n",
        "# [포인트] 출력 파서(Output Parser) 정의\n",
        "# → 모델의 출력을 '쉼표로 구분된 리스트' 형식으로 정리해주는 도구입니다.\n",
        "output_parser = CommaSeparatedListOutputParser()\n",
        "\n",
        "# [포인트] 파서가 요구하는 출력 형식 정보 가져오기\n",
        "# → 이걸 프롬프트에 포함시켜야 모델이 우리가 원하는 형태로 출력해줍니다.\n",
        "format_instructions = output_parser.get_format_instructions()\n",
        "\n",
        "# [포인트] 프롬프트 템플릿 정의 (PromptTemplate)\n",
        "# → 사용자 질문(subject)을 받아서 \"7개의 팀을 보여줘\"라는 요청 문장을 만들고,\n",
        "#    거기에 format_instructions를 자동으로 붙여주는 구조입니다.\n",
        "prompt = PromptTemplate(\n",
        "    template=\"7개의 팀을 보여줘 {subject}.\\n{format_instructions}\",  # 출력 형식까지 포함된 요청 문장 구성\n",
        "    input_variables=[\"subject\"],                                     # 유동적으로 바뀔 입력 값: subject\n",
        "    partial_variables={\"format_instructions\": format_instructions}   # 고정적으로 항상 들어갈 값: 출력 형식 안내\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cO9dylB3Xrg7",
        "outputId": "8dfd96fc-7f35-4299-e0b6-f3aa5a6b0313"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['FC 서울', '전북 현대 모터스', '수원 삼성 블루윙즈', '울산 현대', '포항 스틸러스', '성남 FC', '인천 유나이티드']\n"
          ]
        }
      ],
      "source": [
        "# [포인트] 실제 입력으로 사용할 질문 (사용자 입력 시뮬레이션)\n",
        "query = \"한국의 축구팀은?\"\n",
        "\n",
        "# [포인트] 프롬프트를 완성하고 모델에 전달하여 응답 생성\n",
        "# → 프롬프트는 \"7개의 팀을 보여줘 한국의 야구팀은?\" + 출력형식 안내 문장으로 구성됩니다.\n",
        "output = llm.predict(text=prompt.format(subject=query))\n",
        "\n",
        "# [포인트] 모델의 응답을 파싱(parse)하여 리스트 형태로 정제\n",
        "# → 모델은 문자열을 반환하기 때문에, 그걸 우리가 다루기 쉬운 Python 리스트로 변환합니다.\n",
        "parsed_result = output_parser.parse(output)\n",
        "\n",
        "# 최종 결과 출력\n",
        "# → ['팀1', '팀2', ..., '팀7'] 이런 형태의 리스트가 출력됩니다.\n",
        "print(parsed_result)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G3KgkYbLVVvz",
        "outputId": "474bd751-bceb-46d5-e980-18abcce8bbb0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{\n",
            "  \"teams\": [\n",
            "    {\n",
            "      \"name\": \"FC Seoul\",\n",
            "      \"location\": \"Seoul\"\n",
            "    },\n",
            "    {\n",
            "      \"name\": \"Suwon Samsung Bluewings\",\n",
            "      \"location\": \"Suwon\"\n",
            "    },\n",
            "    {\n",
            "      \"name\": \"Jeonbuk Hyundai Motors\",\n",
            "      \"location\": \"Jeonju\"\n",
            "    },\n",
            "    {\n",
            "      \"name\": \"Pohang Steelers\",\n",
            "      \"location\": \"Pohang\"\n",
            "    },\n",
            "    {\n",
            "      \"name\": \"Ulsan Hyundai\",\n",
            "      \"location\": \"Ulsan\"\n",
            "    },\n",
            "    {\n",
            "      \"name\": \"Incheon United\",\n",
            "      \"location\": \"Incheon\"\n",
            "    },\n",
            "    {\n",
            "      \"name\": \"Seongnam FC\",\n",
            "      \"location\": \"Seongnam\"\n",
            "    }\n",
            "  ]\n",
            "}\n"
          ]
        }
      ],
      "source": [
        "# [포인트] GPT-4 모델을 사용할 수 있게 해주는 LangChain의 클래스\n",
        "# → ChatOpenAI는 OpenAI의 GPT 모델과 쉽게 연결해서 쓸 수 있도록 도와줍니다.\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "\n",
        "# [포인트] 프롬프트 템플릿 생성 도구\n",
        "# → 질문을 구성할 때 변수(예: 사용자 입력)를 포함한 '문장 템플릿'을 만들 수 있습니다.\n",
        "from langchain.prompts import PromptTemplate\n",
        "\n",
        "# [포인트] 모델이 출력한 결과를 JSON 형식으로 정리해주는 파서\n",
        "# → 예: {\"teams\": [\"팀1\", \"팀2\", ..., \"팀7\"]}처럼 구조화된 형태로 결과를 받고 싶을 때 사용합니다.\n",
        "from langchain_core.output_parsers import JsonOutputParser\n",
        "\n",
        "\n",
        "# [1단계] GPT-4 모델 객체 생성\n",
        "# → 모델 이름만 지정하면 자동으로 연결됩니다.\n",
        "llm = ChatOpenAI(model_name='gpt-4')\n",
        "\n",
        "\n",
        "# [2단계] JSON 형식 출력을 위한 파서 설정\n",
        "# → 모델이 응답을 '사람이 읽기 좋은 문장'이 아닌 '기계가 읽기 좋은 구조(딕셔너리)'로 만들어줍니다.\n",
        "output_parser = JsonOutputParser()\n",
        "\n",
        "# [포인트] 파서에게 필요한 출력 형식 설명문을 가져옵니다.\n",
        "# → 이 설명을 GPT에 포함시켜야 모델이 우리가 원하는 JSON 형태로 정확히 응답해줍니다.\n",
        "format_instructions = output_parser.get_format_instructions()\n",
        "\n",
        "\n",
        "# [3단계] 프롬프트(질문) 템플릿 정의\n",
        "# → 사용자가 입력한 주제(subject)에 따라 문장을 만들고, 출력 형식 안내도 같이 넣습니다.\n",
        "prompt = PromptTemplate(\n",
        "    template=\"7개의 팀을 보여줘 {subject}.\\n{format_instructions}\",\n",
        "    input_variables=[\"subject\"],  # 사용자가 입력할 변수 (여기선 \"한국의 야구팀은?\")\n",
        "    partial_variables={\"format_instructions\": format_instructions}  # 고정값: 출력형식 지시문\n",
        ")\n",
        "\n",
        "# [4단계] 실제로 사용자 질문을 입력\n",
        "# → 이 값은 템플릿의 {subject} 자리에 들어가며, 실제 모델 입력이 됩니다.\n",
        "query = \"한국의 축구팀은?\"\n",
        "\n",
        "# [5단계] 최종 프롬프트 문장을 완성하고 모델에 전달\n",
        "# → 완성된 문장은 예를 들어:\n",
        "#   \"7개의 팀을 보여줘 한국의 야구팀은? \\n {여기에 JSON 형식 안내가 포함됨}\"\n",
        "output = llm.predict(text=prompt.format(subject=query))\n",
        "\n",
        "# [6단계] 모델의 응답 출력\n",
        "# → 현재는 출력만 하고 있지만, 이후 이 값을 구조화해서 파싱(parse)할 수 있습니다.\n",
        "print(output)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X2IO3JXwe5IA",
        "outputId": "8b1f754e-4c62-478e-f81c-c6e297a29a58"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "```json\n",
            "{\n",
            "    \"1\": \"폴카닷(Polkadot)은 여러 개의 블록체인을 연결하여 데이터를 교환할 수 있도록 하는 멀티체인 플랫폼이다.\",\n",
            "    \"2\": \"폴카닷은 파라체인(parachain)이라고 불리는 여러 개의 개별 블록체인을 통해 확장성을 높인다.\",\n",
            "    \"3\": \"폴카닷의 네이티브 토큰인 DOT은 네트워크 거버넌스, 스테이킹, 본딩 등에 사용된다.\",\n",
            "    \"4\": \"폴카닷은 이더리움 공동 창립자인 개빈 우드(Gavin Wood)가 설립했다.\",\n",
            "    \"5\": \"폴카닷은 보안성과 상호 운용성을 중점으로 두고 개발된 플랫폼으로, 다른 블록체인 네트워크와의 통합 및 상호 작용을 목표로 한다.\"\n",
            "}\n",
            "```\n"
          ]
        }
      ],
      "source": [
        "# 추가적으로 내가 궁금했던 사항들..\n",
        "\n",
        "llm = ChatOpenAI(model_name='gpt-4o')\n",
        "output_parser = JsonOutputParser()\n",
        "format_instructions = output_parser.get_format_instructions()\n",
        "prompt = PromptTemplate(\n",
        "    template=\"5가지로 정리해주고, 한국어로 출력해줘 {subject}.\\n{format_instructions}\",\n",
        "    input_variables=[\"subject\"],  # 사용자가 입력할 변수 (여기선 \"한국의 야구팀은?\")\n",
        "    partial_variables={\"format_instructions\": format_instructions}  # 고정값: 출력형식 지시문\n",
        ")\n",
        "query = \"폴카닷이라는 암호화폐\"\n",
        "output = llm.predict(text=prompt.format(subject=query))\n",
        "print(output)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0JTVs8WHYqQL"
      },
      "source": [
        "# 체인"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XytE-H7AdCYr"
      },
      "source": [
        "이번 예제의 핵심은 **LLMChain**입니다.  \n",
        "LLMChain은 LangChain의 중심 구조 중 하나로, **프롬프트와 LLM(대형 언어 모델)**을 연결해 주는 ‘하나의 질문-응답 처리 단위’입니다.  \n",
        "간단히 말해, **프롬프트에 입력을 넣고 결과를 받아오는 파이프라인** 역할을 하죠.\n",
        "\n",
        "---\n",
        "\n",
        "### ✅ LLMChain은 왜 필요할까요?\n",
        "\n",
        "우리는 GPT에게 질문할 때 단순히 “이 문장을 설명해줘”라고만 할 수 없습니다.  \n",
        "- 어떤 식으로 질문을 구성할지(프롬프트가 필요함),\n",
        "- 어떤 모델에게 질문을 보낼지(LLM 설정이 필요함),\n",
        "- 입력 값과 출력 값을 어떻게 연결할지(체인 구조가 필요함),\n",
        "\n",
        "이 모든 것을 한 줄로 직접 처리하려면 번거롭고 실수가 많아집니다.\n",
        "\n",
        "그래서 등장한 것이 바로 **LLMChain**입니다.  \n",
        "LLMChain은 이 세 가지 과정을 하나로 묶어서 자동으로 처리해 주기 때문에 **코드가 깔끔하고 논리적인 흐름**을 유지할 수 있게 해 줍니다.\n",
        "\n",
        "---\n",
        "\n",
        "### ⚙️ LLMChain은 내부적으로 어떤 일을 하나요?\n",
        "\n",
        "1. 사용자가 입력한 값(ex. 어떤 문장)을 프롬프트 템플릿에 넣습니다.\n",
        "2. 이 템플릿은 하나의 완성된 질문 문자열로 바뀝니다.\n",
        "3. 완성된 질문은 지정된 GPT 모델(LangChain에서는 ChatOpenAI 등)에 전달됩니다.\n",
        "4. 모델이 응답을 생성합니다.\n",
        "5. LLMChain은 이 결과를 다시 사용자에게 반환합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YzxXbjMcjXV3",
        "outputId": "1093f9f5-b4b8-4704-8de9-ddd28b212be6"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-21-e5dbb8d267e0>:19: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 1.0. Use :meth:`~RunnableSequence, e.g., `prompt | llm`` instead.\n",
            "  chain = LLMChain(llm=llm, prompt=prompt)\n",
            "<ipython-input-21-e5dbb8d267e0>:30: LangChainDeprecationWarning: The method `Chain.run` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
            "  result = chain.run(input_text)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "이 문장은 인공지능이 인간의 학습 방식과 유사하게 정보를 수집하고 이해하며, 이를 바탕으로 문제를 해결하는 능력을 가진 컴퓨터 기술이라는 것을 설명하고 있습니다.\n"
          ]
        }
      ],
      "source": [
        "# [포인트] PromptTemplate은 '질문 틀'을 만드는 도구입니다.\n",
        "# 사용자가 입력한 문장을 어떤 형태의 질문으로 바꿔서 모델에게 보낼지 정의합니다.\n",
        "from langchain import PromptTemplate, LLMChain\n",
        "\n",
        "# [1단계] 프롬프트 템플릿 정의\n",
        "# → GPT에게 어떤 질문을 할 것인지 미리 형식을 정해두는 단계입니다.\n",
        "# 여기서는 사용자가 입력한 문장을 '간단히 설명해 주세요'라는 질문으로 바꾸는 역할을 합니다.\n",
        "prompt = PromptTemplate(\n",
        "    input_variables=['text'],  # 이 템플릿에서 바뀔 부분은 'text'라는 변수입니다.\n",
        "    template=\"다음 문장을 간단히 설명해 주세요: {text}\"  # 실제 프롬프트 형식\n",
        "    # 예: 사용자가 text에 \"인공지능은...\"을 넣으면\n",
        "    # 최종 질문은 → \"다음 문장을 간단히 설명해 주세요: 인공지능은...\"\n",
        ")\n",
        "\n",
        "\n",
        "# [2단계] LLMChain 생성\n",
        "# [포인트] LLMChain은 '프롬프트 + 모델'을 연결해주는 통로 역할을 합니다.\n",
        "# 쉽게 말해, 우리가 만든 프롬프트에 내용을 넣고 모델에게 질문하고, 답을 받아오는 하나의 '질문-응답 흐름'을 만들어 줍니다.\n",
        "chain = LLMChain(llm=llm, prompt=prompt)\n",
        "# → 여기서 llm은 이미 앞에서 정의한 ChatOpenAI(GPT-4 모델) 객체입니다.\n",
        "\n",
        "\n",
        "# [3단계] 설명할 문장을 준비합니다.\n",
        "# → 이 문장이 프롬프트의 {text} 자리에 들어가게 됩니다.\n",
        "input_text = \"인공지능은 인간처럼 학습하고 문제를 해결할 수 있는 컴퓨터 시스템입니다.\"\n",
        "\n",
        "\n",
        "# [4단계] LLMChain 실행\n",
        "# → 입력 문장을 프롬프트에 넣고, 완성된 질문을 GPT에게 보내 답변을 받아옵니다.\n",
        "result = chain.run(input_text)\n",
        "\n",
        "# 예: 완성된 질문은 → \"다음 문장을 간단히 설명해 주세요: 인공지능은 ... 컴퓨터 시스템입니다.\"\n",
        "\n",
        "\n",
        "# [5단계] 결과 출력\n",
        "# → GPT가 생성한 '간단한 설명'을 화면에 출력합니다.\n",
        "print(result)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rwV3iXzBjxxz",
        "outputId": "4bbd442c-2e05-4a75-adbd-baa50a1677f2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Artificial intelligence is a computer system designed to mimic human learning and problem-solving abilities.\n"
          ]
        }
      ],
      "source": [
        "# [포인트] LangChain의 핵심 구성요소들 가져오기\n",
        "from langchain import PromptTemplate, LLMChain         # 프롬프트와 체인 기능을 사용하기 위한 모듈\n",
        "from langchain.chains import SimpleSequentialChain     # 체인들을 순차적으로 연결해주는 기능\n",
        "\n",
        "\n",
        "# [1단계] 프롬프트 1: 문장을 요약하는 템플릿 만들기\n",
        "prompt1 = PromptTemplate(\n",
        "    input_variables=['text'],  # 입력으로 'text'라는 변수를 받습니다.\n",
        "    template=\"다음 문장을 한 문장으로 요약해 주세요: {text}\"\n",
        "    # 사용자의 문장을 받아서 요약 요청 프롬프트를 구성합니다.\n",
        "    # 예: \"인공지능은 ...\" → \"다음 문장을 한 문장으로 요약해 주세요: 인공지능은 ...\"\n",
        ")\n",
        "\n",
        "\n",
        "# [2단계] 프롬프트 2: 요약된 문장을 영어로 번역하는 템플릿 만들기\n",
        "prompt2 = PromptTemplate(\n",
        "    input_variables=['summary'],  # 여기서는 요약된 문장('summary')을 입력으로 받습니다.\n",
        "    template=\"다음 문장을 영어로 번역해 주세요:{summary}\"\n",
        "    # 예: 요약된 문장이 \"인공지능은 컴퓨터 시스템입니다.\"일 경우\n",
        "    # → \"다음 문장을 영어로 번역해 주세요: 인공지능은 컴퓨터 시스템입니다.\"\n",
        ")\n",
        "\n",
        "\n",
        "# [3단계] 체인 1: GPT에게 문장 요약을 요청하는 체인\n",
        "chain1 = LLMChain(llm=llm, prompt=prompt1)\n",
        "# → 첫 번째 체인은 prompt1을 사용하여 입력 문장을 요약합니다.\n",
        "\n",
        "# [4단계] 체인 2: GPT에게 번역을 요청하는 체인\n",
        "chain2 = LLMChain(llm=llm, prompt=prompt2)\n",
        "# → 두 번째 체인은 prompt2를 사용하여 요약된 문장을 영어로 번역합니다.\n",
        "\n",
        "\n",
        "# [5단계] SimpleSequentialChain: 체인 1의 출력을 체인 2의 입력으로 자동 연결\n",
        "simple_chain = SimpleSequentialChain(chains=[chain1, chain2])\n",
        "# [포인트] '요약' → '번역' 순서로 두 작업을 자동으로 이어주는 구조입니다.\n",
        "# 마치 '요약 머신'과 '번역 머신'을 파이프처럼 직렬로 연결한 구조라고 보면 됩니다.\n",
        "\n",
        "\n",
        "# [6단계] 테스트 문장 준비\n",
        "input_text = \"인공지능은 인간의 학습 능력과 문제 해결 능력을 모방한 컴퓨터 시스템입니다.\"\n",
        "\n",
        "\n",
        "# [7단계] 전체 체인 실행\n",
        "# → 요약하고, 이어서 영어로 번역하는 두 작업을 한 번에 처리합니다.\n",
        "result = simple_chain.run(input_text)\n",
        "\n",
        "# 최종 결과 출력\n",
        "print(result)\n",
        "# → 출력 예시: \"Artificial intelligence is a computer system that mimics human learning and problem-solving skills.\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "yGFZ_Lt5kzx1"
      },
      "outputs": [],
      "source": [
        "# chain = prompt1 | llm | prompt2 | llm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "s2kBPL5olXx6"
      },
      "outputs": [],
      "source": [
        "# input_text = \"매우 매우 기분이 좋아서 하늘을 날고 싶다\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "1KErdpJzk_v0"
      },
      "outputs": [],
      "source": [
        "# chain.invoke(input_text)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "7oDTcxmtlyvg"
      },
      "outputs": [],
      "source": [
        "# # 줄바꿈(\\n)을 제거하는 함수 정의\n",
        "# def remove_newlines(text):\n",
        "#     return text.replace('\\n', '')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "0Jmleuhtl7-s"
      },
      "outputs": [],
      "source": [
        "# chain = prompt1 | llm | prompt2 | llm | remove_newlines"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "Sb0N2BtEmATS"
      },
      "outputs": [],
      "source": [
        "# chain.invoke(input_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fTIi4PjP3RhS"
      },
      "source": [
        "# LCEL(LangChain Expression Language)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EN4i4Dtv3Xdk"
      },
      "source": [
        "![image.png](data:image/png;base64,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)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "2pyZS3EH3b5o"
      },
      "outputs": [],
      "source": [
        "# [포인트] PromptTemplate은 '프롬프트(질문)'의 형식을 미리 정해놓는 템플릿입니다.\n",
        "# 템플릿 안의 {topic} 자리에 우리가 원하는 주제를 넣으면 자동으로 질문 문장이 완성됩니다.\n",
        "# 예시: topic이 '축구'면 → \"축구 에 대해 쉽게 설명해주세요.\"라는 문장이 만들어짐.\n",
        "prompt = PromptTemplate.from_template(\"{topic} 에 대해 쉽게 설명해주세요.\")\n",
        "# [포인트] ChatOpenAI는 실제로 GPT 모델(GPT-3.5 또는 GPT-4 등)을 불러와 사용하는 LangChain 객체입니다.\n",
        "# 별도의 설정 없이도 기본값으로 모델을 사용할 수 있게 해줍니다.\n",
        "model = ChatOpenAI()\n",
        "\n",
        "# [포인트] LCEL 문법을 사용해 prompt → model 순으로 연결합니다.\n",
        "# LCEL(LangChain Expression Language)은 파이프(|) 기호를 써서 여러 컴포넌트를 순차적으로 연결하는 방식입니다.\n",
        "# 즉, '프롬프트로 문장 만들기 → 모델에 전달 → 응답 받기' 흐름을 한 줄로 표현할 수 있습니다.\n",
        "chain = prompt | model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ii7Q0bsx3iWI",
        "outputId": "5e7c1fb9-cdfe-481e-c6da-7ddadc206af7"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "AIMessage(content='축구는 두 팀이 경기장에서 공을 발로 차서 상대팀의 골대에 넣어 점수를 얻는 스포츠입니다. 각 팀은 골을 막기 위해 골키퍼를 두고, 경기는 일정 시간 동안 진행됩니다. 공을 차는 것과 상대편 골대로 넣어 득점하는 것이 주요 목표이며, 팀워크와 전략이 중요한 게임입니다. 각 팀은 11명의 선수로 이루어져 있고, 경기는 주로 90분 동안 진행되지만 추가 시간이 붙기도 합니다. 더 많은 골을 넣은 팀이 승리하게 됩니다.', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 220, 'prompt_tokens': 22, 'total_tokens': 242, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run--d7944e67-2c9d-43f2-bc25-9bcf893b501b-0')"
            ]
          },
          "execution_count": 30,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# [포인트] 실제 사용자 입력을 딕셔너리 형식으로 준비합니다.\n",
        "# 'topic' 키는 위 프롬프트 템플릿의 {topic}에 들어갈 값을 의미합니다.\n",
        "input = {\"topic\": \"축구\"}\n",
        "# [포인트] chain.invoke()는 LCEL 체인을 실행하는 함수입니다.\n",
        "# 아래와 같은 과정을 자동으로 처리합니다:\n",
        "# 1. prompt에 '축구'가 들어가서 → \"축구 에 대해 쉽게 설명해주세요.\" 문장이 생성됨\n",
        "# 2. 이 문장이 GPT 모델에게 전달됨\n",
        "# 3. 모델의 응답을 받아 최종 결과로 출력됨\n",
        "chain.invoke(input)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "audydKNmdDCg",
        "outputId": "d94004bb-c9bb-46ab-e0c7-de4825b06248"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "AIMessage(content='축구는 두 팀이 경기장에서 공을 발로 차서 상대팀의 골망에 넣어 득점을 하는 스포츠입니다. 각 팀은 골키퍼를 포함해 11명의 선수로 구성되어 있으며, 경기는 90분 동안 진행됩니다. 목표는 경기 종료 시에 더 많은 골을 넣어 상대팀보다 승리하는 것입니다. 축구는 전 세계적으로 인기 있는 스포츠로 많은 팬들이 경기를 관람하고 선수들의 활약을 응원합니다.', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 188, 'prompt_tokens': 22, 'total_tokens': 210, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run--a6dc952d-6198-4d87-865e-f485491368f2-0')"
            ]
          },
          "execution_count": 31,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# 🔄 위 내용 반복: 동일한 구조로 다시 prompt → model → chain 구성\n",
        "# (의도적으로 반복 작성된 부분이니 강의에서는 생략하거나 복습 개념으로 짚어주면 좋습니다)\n",
        "prompt = PromptTemplate.from_template(\"{topic} 에 대해 쉽게 설명해주세요.\")\n",
        "model = ChatOpenAI()\n",
        "chain = prompt | model\n",
        "input = {\"topic\": \"축구\"}\n",
        "chain.invoke(input)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n_cvcZNZac03"
      },
      "source": [
        "# LCEL이 없는 경우"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "id": "w1jG-dSHaeaN",
        "outputId": "e93dd2fe-0f0b-47dd-f63c-94e5a347119d"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'아이스크림은 주로 디저트로 먹는 얼음과 우유를 주 원료로 만든 음식입니다. 주로 새우고 식당에서, 가정에서 또는 아이스크림 가게에서 즐기실 수 있습니다. \\n\\n아이스크림은 다양한 맛과 텍스처로 제공되며, 초콜릿, 바닐라, 딸기, 크림, 초코칩, 마카다미아너트 등 다양한 플레이버를 갖추고 있습니다. 또한 아이스크림 콘, 아이스크림 케이크, 아이스크림 샌드위치와 같이 다양한 형태로 제공되기도 합니다.\\n\\n아이스크림은 여름철에는 시원하고 상큼한 맛을 낼 뿐만 아니라, 겨울에도 따뜻한 컵에 담아 따뜻한 음료나 과자와 함께 즐기기도 합니다. 그리고 아이스크림은 달콤하고 부드러운 맛으로 사랑받는 디저트로 널리 알려져 있습니다.'"
            ]
          },
          "execution_count": 32,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# ▶ openai 라이브러리를 직접 사용한 방식 (LangChain 없이 직접 API 호출)\n",
        "from typing import List\n",
        "import openai\n",
        "\n",
        "# [포인트] 프롬프트 문장을 직접 문자열로 작성합니다.\n",
        "prompt_template = \"{topic}에 대해 설명해주세요\"\n",
        "\n",
        "# OpenAI 클라이언트 객체 생성 (주의: 실제 API 키가 필요함)\n",
        "client = openai.OpenAI()\n",
        "\n",
        "# [포인트] chat 모델을 직접 호출하는 함수 정의\n",
        "# messages는 GPT 모델에게 전달되는 채팅 형식의 입력이며, 반드시 role과 content가 있어야 합니다.\n",
        "def call_chat_model(messages: List[dict]) -> str:\n",
        "    response = client.chat.completions.create(\n",
        "        model=\"gpt-3.5-turbo\",  # 사용할 모델명\n",
        "        messages=messages,      # 사용자 질문 리스트 전달\n",
        "    )\n",
        "    return response.choices[0].message.content  # 모델 응답 중 첫 번째 메시지 추출\n",
        "\n",
        "# [포인트] 프롬프트에 topic을 넣어서 완성한 후 GPT 모델에게 전달하고 응답을 받는 전체 흐름을 처리합니다.\n",
        "def invoke_chain(topic: str) -> str:\n",
        "    prompt_value = prompt_template.format(topic=topic)  # 예: \"ice cream에 대해 설명해주세요\"\n",
        "    messages = [{\"role\": \"user\", \"content\": prompt_value}]  # 사용자 역할로 메시지 구성\n",
        "    return call_chat_model(messages)\n",
        "\n",
        "# 함수 호출: 'ice cream'이라는 주제를 GPT에게 설명해달라고 요청함\n",
        "invoke_chain(\"ice cream\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N70xLsP5bCcf"
      },
      "source": [
        "# LCEL을 사용한 경우"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "LiOyn6QoapB0",
        "outputId": "e61ab5d7-f410-4e77-9822-6306551dbb44"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'아이스크림은 주로 디저트로 즐기는 음식으로, 주로 냉동 디저트로 만들어진다. 주로 우유나 크림, 설탕, 과일 등의 재료를 섞어 만들고, 그 후 냉동하여 굳혀 만든다. 다양한 맛이 있어 초콜릿, 바닐라, 딸기, 초콜릿칩 등이 있고, 종류도 매우 다양하다. 아이스크림은 뜨거운 날씨에는 시원하게 즐기거나, 디저트로도 즐길 수 있어 매우 인기 있는 음식이다.'"
            ]
          },
          "execution_count": 33,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# ▶ LangChain의 고급 구성: 프롬프트 + 모델 + 출력 파서까지 체이닝\n",
        "from langchain_core.runnables import RunnablePassthrough\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "# [포인트] ChatPromptTemplate: 대화형 GPT에게 적합한 프롬프트 생성 방식\n",
        "prompt = ChatPromptTemplate.from_template(\"{topic}에 대해 설명해주세요\")\n",
        "\n",
        "# GPT 모델 설정\n",
        "model = ChatOpenAI(model=\"gpt-3.5-turbo\")\n",
        "\n",
        "# [포인트] StrOutputParser: 모델의 응답을 문자열 형태로 파싱해주는 도구입니다.\n",
        "output_parser = StrOutputParser()\n",
        "\n",
        "# [포인트] 전체 체인을 구성합니다.\n",
        "# 입력받은 문자열을 topic에 넣고 → 프롬프트 생성 → 모델 실행 → 결과 파싱까지 한 줄로 연결\n",
        "chain = (\n",
        "    {\"topic\": RunnablePassthrough()}  # 입력값 그대로 전달\n",
        "    | prompt                          # 프롬프트에 topic 삽입\n",
        "    | model                           # 모델에 전달\n",
        "    | output_parser                   # 문자열로 결과 추출\n",
        ")\n",
        "\n",
        "# 'ice cream'이라는 입력으로 전체 체인 실행\n",
        "chain.invoke(\"ice cream\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VVYTshDJi0nK"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Ziy3oh5Xur5"
      },
      "source": [
        "## 프롬프트 prompt + 언어모델 model + 출력파서 output parser"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "WOeBlPvbXxRf",
        "outputId": "0a9cbc7d-1884-4943-c4ed-969c6c6d8835"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'지구의 자전 주기는 약 24시간입니다. 이것은 하루 동안 지구가 자전하는 시간을 의미합니다. 지구가 자전하는 동안 태양은 동쪽에서 서쪽으로 움직이는 것처럼 보이는데, 이것이 하루가 지나는 이유입니다.'"
            ]
          },
          "execution_count": 34,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# ▶ 좀 더 특화된 프롬프트 예시: 역할 지시와 질문이 포함됨\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "# [포인트] 프롬프트에 역할(role)을 명확히 지정: \"당신은 천문학 전문가입니다\"\n",
        "prompt = ChatPromptTemplate.from_template(\"You are an expert in astronomy. Answer the question. : {input}\")\n",
        "\n",
        "# GPT 모델 호출 준비\n",
        "llm = ChatOpenAI(model=\"gpt-3.5-turbo-0125\")\n",
        "output_parser = StrOutputParser()\n",
        "\n",
        "# LCEL 체인 구성\n",
        "chain = prompt | llm | output_parser\n",
        "\n",
        "# [포인트] 사용자의 질문을 넣고 체인 실행\n",
        "chain.invoke({\"input\": \"지구의 자전 주기는?\"})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_X6EuuWeXzBS"
      },
      "source": [
        "## 고급(Multiple chain)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "8EuqakGMX19Q",
        "outputId": "62722f57-3463-48a3-be67-6ec8f7c1e218"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'2035년, 지구는 인간이 멸종한 지 오래되어 자연이 인간의 유물들을 덮어가고 있다. 그런데 갑자기 우주선이 지구에 착륙한다. 외계인들이 존재하고 있었던 것이다. 인류는 미래로의 새로운 길을 열기 시작한다.'"
            ]
          },
          "execution_count": 35,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# ▶ 연속 체이닝: 첫 번째 체인의 결과를 두 번째 체인의 입력으로 사용\n",
        "\n",
        "# [포인트] 첫 번째 프롬프트 템플릿을 정의합니다.\n",
        "# {korean_word} 자리에 단어를 넣으면 그 단어와 관련된 30자 분량의 짧은 시나리오를 생성하도록 요청합니다.\n",
        "# 예: korean_word가 \"미래\"라면 → \"미래와 관련된 30자 시나리오를 써라\"라는 문장이 생성됨\n",
        "prompt1 = ChatPromptTemplate.from_template(\"아래 단어와 관련해서 30자 시나리오를 써라 {korean_word}\")\n",
        "\n",
        "# 사용할 GPT 모델을 지정합니다. 여기서는 gpt-3.5-turbo-0125를 사용합니다.\n",
        "llm = ChatOpenAI(model=\"gpt-3.5-turbo-0125\")\n",
        "\n",
        "# [포인트] 첫 번째 체인(chain1)을 구성합니다.\n",
        "# 입력된 korean_word를 기반으로 프롬프트 문장을 만들고 → GPT 모델에 전달한 뒤 → 결과를 문자열로 출력합니다.\n",
        "chain1 = prompt1 | llm | StrOutputParser()\n",
        "\n",
        "# [예시] korean_word에 '미래'를 입력합니다.\n",
        "# 결과적으로 GPT는 \"미래와 관련된 30자 시나리오\"를 생성하게 됩니다.\n",
        "chain1.invoke({\"korean_word\":\"미래\"})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "BzOkkMicX3jz",
        "outputId": "a84c18a3-f471-473c-eb5d-a8287b4e49b3"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'2030년, 로봇, 인간, 인공지능'"
            ]
          },
          "execution_count": 36,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# [포인트] 두 번째 프롬프트 템플릿을 정의합니다.\n",
        "# {english_word} 자리에 문장을 넣으면, 그 문장에서 주요 키워드 3개를 뽑아달라는 요청이 됩니다.\n",
        "# 예: \"로봇이 지배하는 세상에서...\" → 키워드: \"로봇\", \"지배\", \"세상\"\n",
        "prompt2 = ChatPromptTemplate.from_template(\n",
        "    \"아래 내용에서 주요 키워드3개를 추출하라 {english_word}\"\n",
        ")\n",
        "\n",
        "# [핵심 구조] 두 번째 체인(chain2)을 구성합니다.\n",
        "# chain1의 출력값이 자동으로 {english_word} 자리에 들어갑니다.\n",
        "# 즉, ① \"미래\" → 시나리오 생성 → ② 생성된 시나리오 → 키워드 3개 추출 순서로 흐름이 이어집니다.\n",
        "chain2 = (\n",
        "    {\"english_word\": chain1}  # chain1의 출력이 prompt2의 입력으로 자동 전달됨\n",
        "    | prompt2                 # 두 번째 프롬프트로 문장 구성\n",
        "    | llm                     # 모델에 전달\n",
        "    | StrOutputParser()       # 결과를 문자열로 파싱\n",
        ")\n",
        "\n",
        "# [포인트] chain2.invoke()를 호출할 때는 korean_word만 입력합니다.\n",
        "# 내부적으로는 다음과 같은 연쇄 작업이 이뤄집니다:\n",
        "# 1. '미래' → chain1이 시나리오 생성\n",
        "# 2. 생성된 시나리오 → chain2가 주요 키워드 3개 추출\n",
        "# [결과] chain2는 최종적으로 키워드 리스트를 출력합니다.\n",
        "chain2.invoke({\"korean_word\":\"미래\"})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "LGePUiYNj2xe",
        "outputId": "0d2e0425-3ef4-43c7-ca72-244d70d594e1"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'1. 2035년\\n2. 비트코인\\n3. 블록체인\\n4. 금융 시스템\\n5. 혁신적 변화'"
            ]
          },
          "execution_count": 82,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "## 연습\n",
        "prompt1 = ChatPromptTemplate.from_template(\"아래 단어와 관련해서 100자 시나리오를 써라 {korean_word}\")\n",
        "llm = ChatOpenAI(model=\"gpt-3.5-turbo-0125\")\n",
        "chain1.invoke({\"korean_word\":\"비트코인의 미래\"})\n",
        "prompt2 = ChatPromptTemplate.from_template(\n",
        "    \"아래 내용에서 주요 키워드5개를 추출하라 {english_word}\"\n",
        ")\n",
        "chain2 = (\n",
        "    {\"english_word\": chain1}  # chain1의 출력이 prompt2의 입력으로 자동 전달됨\n",
        "    | prompt2                 # 두 번째 프롬프트로 문장 구성\n",
        "    | llm                     # 모델에 전달\n",
        "    | StrOutputParser()       # 결과를 문자열로 파싱\n",
        ")\n",
        "chain2.invoke({\"korean_word\":\"비트코인의 미래\"})\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FeNqc8oGAE4v"
      },
      "source": [
        "## LangChain을 이용한 실시간 응답처리(Streaming)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1RHRgp7KjSnX",
        "outputId": "e9fce9e7-e98b-4caa-c40d-6df903d4ecbb"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-37-684fc57e0f23>:13: LangChainDeprecationWarning: The method `BaseChatModel.__call__` was deprecated in langchain-core 0.1.7 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
            "  resp = chat([\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "축구를 잘하기 위해서는 몇 가지 요령을 따라야 합니다. 아래는 축구를 잘하기 위한 팁들입니다:\n",
            "\n",
            "1. 기본기를 탄탄히 다지세요: 패스, 드리블, 슈팅, 수비 등 기본적인 축구 기술을 충분히 연습하고 익히세요.\n",
            "\n",
            "2. 체력을 키우세요: 축구는 높은 체력을 요구하는 스포츠이므로 꾸준한 유산소 운동을 통해 체력을 키우세요.\n",
            "\n",
            "3. 팀워크를 중요시하세요: 축구는 팀 스포츠이기 때문에 팀원들과의 소통과 협력이 중요합니다. 훈련 중에도 팀원들과 소통하고 함께 훈련하세요.\n",
            "\n",
            "4. 전술을 익히세요: 경기 중에 상황에 맞는 전략을 수립하고 실행하는 것이 중요합니다. 전략을 익히고 훈련하여 경기 중에 효과적으로 활용하세요.\n",
            "\n",
            "5. 결단력을 갖추세요: 축구는 빠른 판단과 결단력이 필요한 스포츠이므로 자신의 판단에 확신을 갖고 행동하세요.\n",
            "\n",
            "6. 훈련에 힘쓰세요: 축구를 잘하기 위해서는 지속적인 훈련이 필요합니다. 꾸준히 훈련하고 발전하는 자세를 갖추세요.\n",
            "\n",
            "이러한 요령을 따르면 축구를 더욱 잘할 수 있을 것입니다. 특히 기본기를 탄탄히 다지고 팀워크를 중요시하는 것이 중요합니다. 축구를 즐기면서 노력하면 더욱 잘할 수 있을 것입니다."
          ]
        }
      ],
      "source": [
        "# langchain과 OpenAI 라이브러리를 사용하여 실시간으로 질문에 답변할 수 있는 챗봇을 구현합니다.\n",
        "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler  # 실시간 출력을 처리하는 모듈을 가져옵니다.\n",
        "from langchain.chat_models import ChatOpenAI  # OpenAI 기반의 챗봇 모델을 사용하기 위한 모듈을 가져옵니다.\n",
        "from langchain.schema import HumanMessage  # 사용자 메시지를 정의하는 데 사용하는 클래스를 가져옵니다.\n",
        "\n",
        "# 챗봇을 설정합니다. 여기서는 OpenAI의 GPT 모델을 사용하며, 실시간으로 대화를 스트리밍합니다.\n",
        "chat = ChatOpenAI(\n",
        "    streaming=True,  # 'streaming'을 True로 설정해 실시간으로 대화의 결과를 받아볼 수 있습니다.\n",
        "    callbacks=[\n",
        "        StreamingStdOutCallbackHandler()  # 실시간으로 결과를 콘솔에 출력할 수 있도록 콜백 핸들러를 추가합니다.\n",
        "    ])\n",
        "# 챗봇에게 질문을 보내고 답변을 받습니다.\n",
        "resp = chat([\n",
        "    HumanMessage(content=\"축구 잘하는 법을 알려주세요\")  # 사용자의 질문을 정의합니다.\n",
        "])\n",
        "# 위 코드는 사용자의 질문에 대한 답변을 실시간으로 콘솔에 출력합니다.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZuSYNidHYn9N",
        "outputId": "8652cec0-5636-4be0-a308-db6fbdc416c8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "content='축구를 잘하기 위해서는 몇 가지 중요한 요소가 있습니다. 아래는 축구를 잘하기 위한 팁들입니다:\\n\\n1. 기본기를 탄탄하게 다지세요: 축구를 잘하기 위해서는 기본기가 중요합니다. 드리블, 패스, 슈팅, 수비 등의 기본기를 탄탄하게 다지는 것이 중요합니다.\\n\\n2. 체력을 키우세요: 축구는 고강도의 육체 활동이 요구되므로 좋은 체력이 필요합니다. 꾸준한 유산소 운동과 근력 운동을 통해 체력을 키워주세요.\\n\\n3. 팀워크를 강화하세요: 축구는 팀 스포츠이기 때문에 팀원들과의 소통과 협동이 중요합니다. 팀원들과의 원활한 커뮤니케이션과 협동을 강화하세요.\\n\\n4. 전략을 세우세요: 축구는 전략적인 게임이기 때문에 전략을 잘 세우는 것이 중요합니다. 상대팀을 분석하고 전략을 세워 경기에 임하세요.\\n\\n5. 꾸준한 연습이 필요합니다: 축구를 잘하기 위해서는 꾸준한 연습이 필요합니다. 매일 조금씩이라도 연습을 하고 기본기를 탄탄하게 다지세요.\\n\\n이러한 팁들을 참고하여 축구를 잘하실 수 있을 것입니다. 특히, 꾸준한 노력과 열정이 중요합니다. 축구를 사랑하고 열정을 가지고 노력하면 분명히 성과를 얻을 수 있을 것입니다.' additional_kwargs={} response_metadata={'token_usage': {'completion_tokens': 509, 'prompt_tokens': 21, 'total_tokens': 530, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None} id='run--e0cc8a96-332e-49df-8596-568858602fc9-0'\n"
          ]
        }
      ],
      "source": [
        "##비교 예시(스트리밍이 없는 경우)\n",
        "\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.schema import HumanMessage\n",
        "\n",
        "chat = ChatOpenAI()\n",
        "\n",
        "resp = chat([\n",
        "    HumanMessage(content=\"축구 잘하는 법을 알려주세요\")\n",
        "])\n",
        "\n",
        "print(resp)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ojOwxHIlXTrP"
      },
      "source": [
        "# 메모리(기억)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "bOIYCf_4_cbh"
      },
      "outputs": [],
      "source": [
        "# ! pip install openai\n",
        "# ! pip install langchain\n",
        "# ! pip install -U langchain-community"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Ca-4WYnX0wc"
      },
      "source": [
        "### ChatMessageHistory\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mzqb0IWrt_gs"
      },
      "source": [
        "# 🤖 GPT 대화 기억 방식 비교: ChatMessageHistory vs ConversationChain\n",
        "\n",
        "GPT는 기본적으로 **기억력이 없습니다.**  \n",
        "하지만 LangChain을 사용하면 GPT가 **이전 대화를 기억**하게 만들 수 있습니다.  \n",
        "이때 사용하는 대표적인 두 가지 도구가 바로 `ChatMessageHistory`와 `ConversationChain`입니다.\n",
        "\n",
        "---\n",
        "\n",
        "## 🔹 1. ChatMessageHistory: 수동 대화 저장 방식\n",
        "\n",
        "### 📌 개념\n",
        "- GPT와 나눈 대화를 **직접 저장**해주는 \"대화 일기장\"입니다.\n",
        "- 이전 대화를 하나하나 수동으로 기록하며, 이를 바탕으로 문맥을 인식시킵니다.\n",
        "\n",
        "✅ 특징\n",
        "메시지를 직접 입력해서 저장\n",
        "\n",
        "사용자가 어떤 메시지를 남길지 선택과 제어가 가능\n",
        "\n",
        "복잡한 대화 흐름, 조건 분기, 커스터마이징에 유리\n",
        "\n",
        "단점: 자동으로 저장되지 않음 → 직접 추가해야 함\n",
        "\n",
        "\n",
        "\n",
        "##  2. ConversationChain: 자동 대화 흐름 관리\n",
        "\n",
        "📌 개념\n",
        "대화 내용을 LangChain이 자동으로 기억하면서 문맥 기반 응답을 만들어주는 도구입니다.\n",
        "\n",
        "predict() 함수로 입력만 하면, 이전 대화까지 알아서 저장하고 이어서 대답합니다.\n",
        "\n",
        "✅ 특징\n",
        "사용자의 입력과 GPT의 응답이 자동으로 메모리에 저장\n",
        "\n",
        "빠르게 챗봇을 만들거나 테스트할 때 유용\n",
        "\n",
        "단점: 내부 흐름이 자동이라, 세부 제어가 어려움\n",
        "\n",
        "📊 두 방식 비교 요약\n",
        "항목\tChatMessageHistory\tConversationChain\n",
        "🎯 목적\t수동 메시지 기록\t자동 대화 기억 관리\n",
        "🧠 기억 방식\t사용자가 직접 입력\t자동 저장\n",
        "🔧 제어력\t높음 (직접 조작 가능)\t낮음 (자동 처리됨)\n",
        "🧰 사용 시점\t복잡한 커스터마이징, 조건 분기\t간단한 챗봇, 테스트, 데모\n",
        "📦 메시지 추가\tadd_user_message() 등 명시적 입력\tpredict()만 사용하면 자동 처리\n",
        "\n",
        "🎯 정리\n",
        "\n",
        "\n",
        "\n",
        "ChatMessageHistory는 GPT와의 대화를 사용자가 직접 저장하고 관리하는 방식입니다.\n",
        "→ 섬세한 제어가 필요한 상황에 적합합니다.\n",
        "\n",
        "\n",
        "\n",
        "ConversationChain은 LangChain이 자동으로 대화를 기억하고 이어주는 도구입니다.\n",
        "→ 빠르게 챗봇을 만들고 싶을 때, 간단한 흐름을 다룰 때 적합합니다.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HIWxPE5v_nna",
        "outputId": "5413d8b1-df95-4693-b575-752cda7efc08"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "현재 대화 내역: AI: 안녕하세요!\n",
            "Human: 프랑스의 수도는 어디인가요?\n",
            "AI: 프랑스의 수도는 파리입니다.\n"
          ]
        }
      ],
      "source": [
        "from langchain.memory import ChatMessageHistory\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "\n",
        "# [포인트] ChatOpenAI는 GPT 모델을 사용하기 위한 인터페이스입니다.\n",
        "# 이 객체를 통해 GPT에게 질문을 보내고 응답을 받을 수 있습니다.\n",
        "chat = ChatOpenAI()\n",
        "\n",
        "# [포인트] ChatMessageHistory는 '대화 기록'을 저장하는 메모리 객체입니다.\n",
        "# GPT와의 이전 대화들을 기억하게 하려면, 이 객체를 활용해야 합니다.\n",
        "# 여기 저장된 메시지들이 GPT 모델에 함께 전달되어, 문맥을 이해하게 도와줍니다.\n",
        "history = ChatMessageHistory()\n",
        "\n",
        "# [포인트] AI가 먼저 인사하는 메시지를 추가합니다.\n",
        "# 역할: 'AI'로 메시지를 남기며, 초기 대화의 시작을 만듭니다.\n",
        "history.add_ai_message(\"안녕하세요!\")\n",
        "\n",
        "# [포인트] 사용자가 질문한 내용을 기록합니다.\n",
        "# 이번엔 프랑스의 수도에 대한 질문을 추가했습니다.\n",
        "history.add_user_message(\"프랑스의 수도는 어디인가요?\")\n",
        "\n",
        "# [포인트] AI가 사용자 질문에 답변한 내용을 기록합니다.\n",
        "# 예시 응답: \"프랑스의 수도는 파리입니다.\"\n",
        "history.add_ai_message(\"프랑스의 수도는 파리입니다.\")\n",
        "\n",
        "# [포인트] 지금까지 누적된 대화 내용을 출력해봅니다.\n",
        "# ChatMessageHistory 객체에는 지금까지 오간 메시지들이 차곡차곡 쌓여있습니다.\n",
        "print(\"현재 대화 내역:\", history)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XwxhKkoFB5uX",
        "outputId": "f1dfbb4a-567f-4ded-ba00-cb61fe31c32e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "프랑스의 수도는 파리입니다.\n"
          ]
        }
      ],
      "source": [
        "# [포인트] 사용자가 새로운 질문을 합니다.\n",
        "# 질문 내용: 앞에서 이야기한 수도가 무엇이었는지 다시 확인 요청\n",
        "# 중요한 점은 이 질문이 앞선 대화 맥락을 기반으로 하고 있다는 것입니다.\n",
        "new_question = \"앞의 대화에서 말한 수도를 다시 한번 말해주세요.\"\n",
        "history.add_user_message(new_question)\n",
        "\n",
        "# [포인트] 지금까지 누적된 메시지들(history.messages)을 기반으로 GPT에게 질문을 보냅니다.\n",
        "# GPT는 지금까지의 대화 전체를 보고, 그 문맥을 바탕으로 새 질문에 답변하게 됩니다.\n",
        "response = chat(history.messages)\n",
        "\n",
        "# [포인트] GPT가 응답한 내용을 출력합니다.\n",
        "# 앞의 대화 흐름을 인식했기 때문에, '파리입니다'와 같은 응답이 예상됩니다.\n",
        "print(response.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UNbpF_4LYiWa"
      },
      "source": [
        "[링크 텍스트](https://)### ConversationChain"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TTRiFPNFQkP9"
      },
      "source": [
        "ConversationChain은 문맥 기억 기능이 자동 포함된 체인입니다.\n",
        "→ 이전 입력을 따로 저장하거나 전달할 필요 없이, GPT가 흐름을 이어받아 응답할 수 있게 도와줍니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vDpLMJPKCnnA"
      },
      "source": [
        "주요 특징 및 동작\n",
        "ConversationChain 사용: 이 코드에서는 ConversationChain 클래스를 사용하여 대화를 관리합니다.\n",
        "LLM 설정: ChatOpenAI 객체를 생성하고 ConversationChain에 전달하여 사용합니다.\n",
        "predict 메소드 사용: predict 메소드를 사용하여 대화를 진행합니다. 입력된 내용을 기반으로 AI가 응답을 생성합니다.\n",
        "Verbose 모드: verbose=True 설정으로 대화의 자세한 진행 과정을 출력합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 803
        },
        "id": "Dsmq9RqQCH3M",
        "outputId": "f06aa171-b231-483d-b513-d47e7d56d68e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "\n",
            "Human: 진규는 사과가 3개를 가지고 있습니다.\n",
            "AI:\u001b[0m\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-42-bff9abd7fb18>:15: LangChainDeprecationWarning: The class `ConversationChain` was deprecated in LangChain 0.2.7 and will be removed in 1.0. Use :meth:`~RunnableWithMessageHistory: https://python.langchain.com/v0.2/api_reference/core/runnables/langchain_core.runnables.history.RunnableWithMessageHistory.html` instead.\n",
            "  conversation = ConversationChain(llm=llm, verbose=True)\n",
            "/usr/local/lib/python3.11/dist-packages/pydantic/main.py:253: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
            "  validated_self = self.__pydantic_validator__.validate_python(data, self_instance=self)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "Human: 진규는 사과가 3개를 가지고 있습니다.\n",
            "AI: 그렇군요, 진규 씨가 사과를 3개 가지고 있다는 것을 알게 되었습니다. 이 정보를 어떻게 활용하면 좋을까요?\n",
            "Human: 영희는 배를 5개 가지고 있습니다.\n",
            "AI:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "Human: 진규는 사과가 3개를 가지고 있습니다.\n",
            "AI: 그렇군요, 진규 씨가 사과를 3개 가지고 있다는 것을 알게 되었습니다. 이 정보를 어떻게 활용하면 좋을까요?\n",
            "Human: 영희는 배를 5개 가지고 있습니다.\n",
            "AI: 알겠습니다, 영희 씨는 배를 5개 가지고 있다는 정보를 기억하겠습니다. 이제 진규 씨와 영희 씨가 각각 어떤 과일을 얼마나 가지고 있는지 알게 되었네요. 이 정보를 통해 무엇을 도출하고 싶으신가요?\n",
            "Human: 진규와 영희가 가지고 있는 과일수는?\n",
            "AI:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'진규 씨가 가지고 있는 사과는 3개이고, 영희 씨가 가지고 있는 배는 5개입니다. 따라서 진규와 영희가 가지고 있는 과일의 총 수는 8개입니다.'"
            ]
          },
          "execution_count": 42,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain import ConversationChain\n",
        "\n",
        "# [포인트] ChatOpenAI: GPT-4 모델을 불러오기 위한 LangChain 객체입니다.\n",
        "# temperature=0 → 창의성 없이 '사실 기반의 정확한 응답'을 원할 때 사용합니다.\n",
        "# model_name='gpt-4' → 사용할 GPT 모델 버전 지정\n",
        "llm = ChatOpenAI(\n",
        "    temperature=0,\n",
        "    model_name='gpt-4',\n",
        ")\n",
        "\n",
        "# [포인트] ConversationChain: GPT와의 대화를 기억하면서 이어가는 객체입니다.\n",
        "# 내부적으로 '대화 기록 메모리'를 자동으로 관리해줍니다.\n",
        "# verbose=True → 실행 로그를 자세히 출력하므로 디버깅이나 학습 시 매우 유용합니다.\n",
        "conversation = ConversationChain(llm=llm, verbose=True)\n",
        "\n",
        "# [1단계 입력] 사용자가 첫 번째 문장을 입력합니다.\n",
        "# \"진규는 사과가 3개를 가지고 있습니다.\" → 이 정보는 대화 기록에 저장됩니다.\n",
        "conversation.predict(input=\"진규는 사과가 3개를 가지고 있습니다.\")\n",
        "\n",
        "# [2단계 입력] 두 번째 문장이 이어집니다.\n",
        "# \"영희는 배를 5개 가지고 있습니다.\" → 이 문장도 마찬가지로 기억됩니다.\n",
        "conversation.predict(input=\"영희는 배를 5개 가지고 있습니다.\")\n",
        "\n",
        "# [3단계 질문] 이전 두 문장을 바탕으로 질문을 합니다.\n",
        "# [포인트] ConversationChain은 앞서 입력된 내용을 기억하고 있으므로,\n",
        "# \"진규와 영희가 가지고 있는 과일수는?\"이라는 질문에 대해\n",
        "# → 3 + 5 = 8이라는 답을 도출할 수 있습니다.\n",
        "conversation.predict(input=\"진규와 영희가 가지고 있는 과일수는?\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MYlZLNbyQy0p"
      },
      "source": [
        "## 🧠 LangChain 기억 기능 비교: `ChatMessageHistory` vs `ConversationChain`\n",
        "\n",
        "| 항목                     | `ChatMessageHistory` 방식                           | `ConversationChain` 방식                        |\n",
        "|--------------------------|----------------------------------------------------|------------------------------------------------|\n",
        "| ✅ 대화 기록 방식         | 수동으로 AI/사용자 메시지를 직접 추가               | 자동으로 이전 대화가 내부적으로 저장됨         |\n",
        "| ✅ 메시지 전달 방식       | `.messages`로 GPT에게 명시적으로 전달               | `predict(input=...)`만 호출하면 자동 전달됨     |\n",
        "| ✅ 코드 제어 수준         | 높음 (디테일하게 기록 조작 가능)                   | 낮음 (직관적이지만 내부 메모리 설정 제한적)     |\n",
        "| ✅ 사용 난이도            | 초보자에겐 약간 복잡                                | 매우 쉬움 (초보자에게 적합)                     |\n",
        "| ✅ 적합한 상황            | 디버깅, 테스트, 커스터마이징, 대화 흐름 분석        | 간단한 챗봇 구현, 빠른 프로토타이핑              |\n",
        "| ✅ 사용 예시 코드        | `history.add_user_message()`, `chat(history.messages)` | `conversation.predict(input=\"...\")`             |\n",
        "\n",
        "### 🔎 요약\n",
        "- **정밀 제어가 필요할 땐** → `ChatMessageHistory`\n",
        "- **빠르고 간단한 챗봇 만들 땐** → `ConversationChain`\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M9yoJ5_4lA77"
      },
      "source": [
        "# RunnablePassthrough"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K5wzTNhjlDvV"
      },
      "source": [
        "아래 코드는 GPT 모델에게 숫자를 입력하면 그 숫자의 10배를 계산해서 응답하도록 구성된 예제입니다. 먼저 \"숫자의 10배는?\"이라는 질문 형식을 만들고, 그 형식에 입력값을 채워 GPT에게 전달합니다. 여기서 핵심은 `RunnablePassthrough`라는 도구인데, 이걸 사용하면 숫자 하나만 전달해도 자동으로 딕셔너리 형태(예: {\"num\": 10})로 바꿔주는 역할을 합니다. 덕분에 코드를 더 간단하고 직관적으로 작성할 수 있습니다. 비유하자면, 첫 번째 코드는 사용자가 직접 서류를 써야 했다면, 이 코드는 숫자만 말하면 서류가 자동으로 작성되는 구조입니다.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xXuvzyc6lO64"
      },
      "source": [
        "### RunnablePassthrough사용하지 않는 경우"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "RWVyWo2llMNd"
      },
      "outputs": [],
      "source": [
        "from langchain_core.prompts import PromptTemplate\n",
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "# [포인트] PromptTemplate: GPT에게 질문할 문장의 '틀(template)'을 정의합니다.\n",
        "# 여기서는 num이라는 변수를 받아서 \"num의 10배는?\"이라는 문장을 만들어냅니다.\n",
        "prompt = PromptTemplate.from_template(\"{num}의 10배는?\")\n",
        "\n",
        "# [포인트] ChatOpenAI: GPT-4 또는 GPT-3.5 모델을 실제로 호출하는 객체입니다.\n",
        "# temperature=0 → 창의성 없이 '정확하고 일관된 답변'을 원할 때 설정합니다.\n",
        "llm = ChatOpenAI(temperature=0)\n",
        "\n",
        "# [포인트] prompt | llm : 파이프(|) 연산자를 사용해 '프롬프트 → GPT' 순서로 연결된 체인을 만듭니다.\n",
        "# 즉, 프롬프트에 값을 넣고 → GPT로 넘겨 → 답변을 받는 전체 과정이 chain으로 구성됩니다.\n",
        "chain = prompt | llm\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8eOmlhm7EnMZ",
        "outputId": "6e511a58-58a4-460f-e141-707ce78794f0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "50\n",
            "100\n",
            "150입니다.\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# [포인트] .invoke({...}): chain을 실행하여 실제 결과를 받아옵니다.\n",
        "# 여기선 num=5를 전달 → \"5의 10배는?\" 생성 → GPT 응답\n",
        "result = chain.invoke({\"num\": 5})\n",
        "print(result.content)  # 출력 예상: \"50입니다.\"\n",
        "\n",
        "# num=10을 입력하여 같은 방식으로 체인 실행\n",
        "result = chain.invoke({\"num\": 10})\n",
        "print(result.content)  # 그런데 여기서도 \"50입니다.\"가 반복된다면 → 프롬프트 사용 방식 점검 필요\n",
        "\n",
        "# num=15 실행\n",
        "result = chain.invoke({\"num\": 15})\n",
        "print(result.content)  # 역시 같은 출력이 반복될 수 있음 → 아래 코드와 비교하면서 문제를 확인해봅니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YPetP-kilUGR"
      },
      "source": [
        "### RunnablePassthrough사용하는 경우"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "6003mXhrlONi"
      },
      "outputs": [],
      "source": [
        "from langchain_core.prompts import PromptTemplate\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_core.runnables import RunnablePassthrough\n",
        "\n",
        "# [포인트] PromptTemplate: 앞과 동일하게 '문장 템플릿'을 정의합니다.\n",
        "prompt = PromptTemplate.from_template(\"{num}의 10배는?\")\n",
        "\n",
        "# [포인트] LLM 정의: 실제 GPT 모델 연결\n",
        "llm = ChatOpenAI(temperature=0)\n",
        "\n",
        "# [포인트] RunnablePassthrough: 입력값을 그대로 전달해주는 '입력 전달자' 역할을 합니다.\n",
        "# 여기서는 \"10\" 같은 단일 숫자를 받아, 그것을 {\"num\": 10} 형태의 딕셔너리로 만들어주는 효과를 줍니다.\n",
        "# 예를 들어 invoke(10) → {\"num\": 10}으로 바꿔줍니다.\n",
        "\n",
        "# [핵심 구조] runnable_chain = {\"num\": RunnablePassthrough()} | prompt | llm\n",
        "# 1. 입력값을 {\"num\": 값} 형태로 가공\n",
        "# 2. prompt로 전달 → \"값의 10배는?\" 문장 생성\n",
        "# 3. llm(GPT)에게 질문 전달 → 응답 받기\n",
        "runnable_chain = {\"num\": RunnablePassthrough()} | prompt | llm\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WRes-7xtEgtv",
        "outputId": "acf02aff-c919-4b69-c8df-7b1cb473c303"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "100\n",
            "150입니다.\n",
            "200입니다.\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# [포인트] invoke(10): 이제 단순 숫자 10만 넣어도 자동으로 {\"num\": 10} 처리됨\n",
        "result = runnable_chain.invoke(10)\n",
        "print(result.content)  # 출력 예상: \"100입니다.\"\n",
        "\n",
        "# 동일한 방식으로 숫자 15 입력\n",
        "result = runnable_chain.invoke(15)\n",
        "print(result.content)  # 출력: \"150입니다.\"\n",
        "\n",
        "# 20 입력\n",
        "result = runnable_chain.invoke(20)\n",
        "print(result.content)  # 출력: \"200입니다.\"\n",
        "\n",
        "# ✅ 정리:\n",
        "# - 앞의 코드에서는 딕셔너리 형식으로 직접 {\"num\": 값}을 넣어줘야 했습니다.\n",
        "# - 하지만 여기서는 RunnablePassthrough를 활용해 숫자만 넣으면 자동으로 키-값 형태로 바뀝니다.\n",
        "# - 따라서 훨씬 직관적이고 깔끔하게 체인을 실행할 수 있습니다.\n",
        "\n",
        "# 💡 비유로 설명하자면:\n",
        "# 앞 코드는 \"준비된 서류 양식에 값을 직접 써야 하는 방식\"이라면,\n",
        "# 여긴 \"숫자만 말하면 알아서 서류를 만들어주는 방식\"입니다.\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "0_zEBrEpyoKu"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
